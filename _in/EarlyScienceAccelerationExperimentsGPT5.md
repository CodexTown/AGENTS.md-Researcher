Early science acceleration experiments with GPT-5

Sébastien Bubeck1, Christian Coester2, Ronen Eldan1, Timothy Gowers3, Yin Tat Lee1,
Alexandru Lupsasca1,4, Mehtaab Sawhney5, Robert Scherrer4, Mark Sellke1,6,
Brian K. Spears7, Derya Unutmaz8, Kevin Weil1, Steven Yin1, Nikita Zhivotovskiy9

1OpenAI
2University of Oxford
3Collège de France and University of Cambridge
4Vanderbilt University
5Columbia University
6Harvard University
7Lawrence Livermore National Laboratory
8The Jackson Laboratory
9University of California, Berkeley

November 20, 2025

Abstract

AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware
of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5
produced new, concrete steps in ongoing research across mathematics, physics, astronomy,
computer science, biology, and materials science. In these examples, the authors highlight how
AI accelerated their work, and where it fell short; where expert time was saved, and where
human input was still key. We document the interactions of the human authors with GPT-5,
as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new
results in mathematics (carefully verified by the human authors), underscoring how GPT-5
can help human mathematicians settle previously unsolved problems. These contributions are
modest in scope but profound in implication, given the rate at which frontier AI is progressing.

Contents

Introduction

I

Independent rediscovery of known results at the scientific frontier
I.1

Improved step-size condition in a recent convex optimization result – Sébastien
Bubeck . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
I.2 Discovering new black hole symmetries with GPT-5 – Alex Lupsasca . . . . .
I.3 Mechanistic analysis and outcome prediction for in vitro immune system exper-
iments using GPT-5 Pro – Derya Unutmaz, M.D. . . . . . . . . . . . . . . . .

II Deep literature search

II.1 From density estimation and convex geometry to multi-objective optimization –
Nikita Zhivotovskiy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
II.2 Erdős problems (part 1/2) – Mehtaab Sawhney and Mark Sellke
II.3 Clique-avoiding codes: a cautionary tale . . . . . . . . . . . . . . . . . . . . .

III Working in tandem with AI

III.1 Recent experiences of using LLMs as research partners – Timothy Gowers . .
III.2 Power spectra of gravitational radiation from cosmic strings – Robert Scherrer
III.3 AI-assisted reduced-physics modeling of thermonuclear burn propagation – Brian
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Keith Spears

IV New scientific results obtained with AI

IV.1 Erdős problems (part 2/2) – Mehtaab Sawhney and Mark Sellke
. . . . . . .
IV.2 New online algorithms lower bounds – Christian Coester . . . . . . . . . . . .
IV.3 Inequalities on subgraph counts in trees – Sébastien Bubeck, Mark Sellke, and
Steven Yin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
IV.4 COLT problem on dynamic networks – Sébastien Bubeck, Mark Sellke, and
Steven Yin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Conclusion

2

3

3
7

11

21

21
24
28

31
31
37

40

53
53
61

69

75

82

1

Introduction

Over the past few years, large language models have become increasingly useful for tasks such
as writing, programming, and planning. More recently, they have started to be capable of
contributing intellectually to scientific research. With GPT-5, we see early signs that, guided
by an expert, the model can propose helpful ideas, perform deep literature searches, and even
produce complete new proofs. This report documents internal and external cases in which
GPT-5 contributed to scientific progress, and how it fell short. Our aim is to highlight what is
possible today, what is still out of reach, and implications for the future of scientific research.
We focus on concrete examples spanning mathematics, physics, astronomy, computer
science, biology, and materials science. Since the subject matter varies widely by section,
each begins with an explanation of the background and motivation before discussing the
contribution made by GPT-5. We also provide ChatGPT conversation transcripts when
possible, and describe the human input that was needed, in order to make clear where the
model currently adds value and where expert oversight remains essential. Several related
accounts in a similar spirit have also surfaced in recent months, including [FK25; DMN25;
IX25; AM25; JR25; Sal25; Geo+25]. We note that the latter paper, on AlphaEvolve by Google,
is slightly different in flavor, as it focuses on search problems with a well-defined objective
function that can be hill-climbed. By contrast, we (and the other cited papers) are focused
on a general-purpose system that can answer any type of query. The two approaches are
complementary, each providing unique advantages to scientists.

We are also careful to point out the limits of current AI models. GPT-5 is imperfect: it can
confidently make mistakes, ardently defend them, and confuse itself (and us) in the process.
Results may depend on fine details of both the initial prompt and follow-up responses, and
thus can be challenging to reproduce. Despite these limitations, we see real progress. GPT-5
can search broad conceptual spaces, integrate diverse information sources, and iterate quickly.
It can tirelessly propose new ideas, help turn an imprecise idea into a concrete result, and
sanity-check or extend a given line of thought. In some areas, such as literature search, it is
often uniquely effective.

Our aim is not to claim more than the evidence allows. It is to show, with specific examples,
what GPT-5 can and cannot do today, and to give a clear path for how researchers can use
it to accelerate scientific discovery while keeping standards high. We believe GPT-5 already
provides substantial value for scientific researchers today, and will become an even more
powerful tool tomorrow. The rest of the paper is organized as follows:

• Chapter I collects examples in which GPT-5 independently rediscovers known results at

the research frontier in math, physics, and biology.

• Chapter II highlights the ability of GPT-5 to perform “deep literature search”. Its ability
to focus on the core concepts rather than the words used to describe them surmounts
barriers of language between scientific disciplines to uncover seemingly forgotten or
hard-to-find connections.

• Chapter III displays examples of human researchers working in tandem with GPT-5 to

accelerate their research workflow.

• Chapter IV presents examples of GPT-5 obtaining novel research-level results.

2

Chapter I

Independent rediscovery of known
results at the scientific frontier

I.1 Improved step-size condition in a recent convex optimiza-

tion result – Sébastien Bubeck

This section can be viewed as a warm-up for the rest of the paper. The experiment was
conducted on August 20th 2025, at a time when it was not yet clear whether GPT-5 could be
used to push the frontier in research level questions. The goal of this experiment was more
modest than what is described in several of the other sections, namely it was merely to see if
GPT-5 could reproduce the main result of a recent paper, specifically the main theorem in
[BSZ25]. This particular paper was chosen for several reasons:

• First, obviously, the paper is not in the training data for GPT-5 as it is too recent. But
even more importantly, the main theorem is about a genuinely new question for a very
classical field (convex optimization), and in particular no trace of this question can be
found in the training data either.

• The paper has three versions on arxiv (v1, v2, v3), and crucially v1 presents a suboptimal
result which is then refined (and made optimal) in v2. The challenge I was interested in
is: given v1, can GPT-5 rederive v2 ?

• In particular, if GPT-5 could rederive v2 from v1, it would indicate that it could have
counterfactually accelerated the scientific discovery process. As we will see, GPT-5 did
not manage to fully rederive the v2 result, but it basically went half-way between v1
and v2. (See also the comment at the end of this section on deriving v2 from scratch
with models that can reason for longer.)

Remarkably, this was my first attempt at an experiment of this type (i.e., I did not try

any other paper than [BSZ25]).

I.1.1 Description of the problem

The most basic algorithm to optimize an objective function f : Rd → R is gradient descent
with constant step-size η, where the iterates are given by

xn+1 = xn − η∇f (x).

3

A classical result in convex optimization is that, for smooth convex functions, convergence
happens if and only if the step size is smaller than 2/L where L is the smoothness constant (i.e.,
the Lipschitz constant of the gradient map x 7→ ∇f (x)). In [BSZ25] the authors study a new
variant of the convergence question, which as far as we know (both the authors of that paper
and the present author) was never studied before: instead of merely asking for convergence,
under what condition is the curve traced by the visited objective function values itself convex,
i.e., when is the piecewise linear function going through the points {(n, f (xn))}n∈N a convex
function itself?

In v1 of the paper, the authors prove that η ≤ 1/L is a sufficient condition, and they also
show that η ≤ 1.75/L is a necessary condition. In v2 of the paper, they close this gap and
show that in fact η ≤ 1.75/L is indeed sufficient.

I.1.2 GPT-5 acting as a collaborator: from v1 getting close to v2

Attachment: [2503.10138v1.pdf]
Can you improve the condition on the step-size in Theorem 1? I
don’t want to add any more hypotheses, I just want you to work
to improve the step-size condition under the same assumptions as
Theorem 1.

[Reasoned for 17m 35s]
Yes. Under the same assumptions as Theorem 1 (convex L-smooth f and fixed step
size for gradient descent), you can enlarge the guaranteed-convexity window from
η ≤ 1/L to

η ≤

3
2L

i.e., the optimization curve n 7→ f (xn) is convex for any step size η ∈ (0, 3/(2L)]. This
strictly improves Theorem 1’s bound and still lies below the non-convexity regime
exhibited in Theorem 2. See Theorem 1 and Theorem 2 (pp. 4-6) for the problem
setup and statements referenced below.

Figure I.1: The interaction with GPT-5 Pro to derive the improved bound.

As discussed above, the experiment I was interested in is given v1, can GPT-5 rederive
v2 ? To check this, I uploaded the PDF of the v1 paper, and simply asked GPT-5 (Pro) to try
to improve the result. The entire interaction is captured in Figure I.1 above. In particular,
one can see that GPT-5 claims to have improved the condition from η ≤ 1/L to η ≤ 1.5/L,
thus approaching the optimal bound (but not quite getting there) of η ≤ 1.75/L. But is this
claim substantiated? It is indeed, and the proof given by GPT-5 is shown in Figure I.2, which
the present author has verified to be correct. Moreover, I inspected the reasoning traces to
make sure that no web search was performed (as this could have revealed the existence of v2),
and furthermore the proof given by GPT-5 is quite different from the one in v2. Indeed, the
GPT-5 proof can be viewed as a more canonical variant of the v1 proof, whereas the (human)

4

v2 proof requires a clever weighting of different inequalities for certain cancellations to happen.

To say it plainly, such a result (improving from 1/L to 1.5/L) could probably have been
achieved by some experts in the field in a matter of hours, and likely for most experts it would
have taken a few days. This is the type of science acceleration that we will see time and again
in this report.

We end this section by noting that our internal models, which can think for a few hours,
were able to derive the optimal bound 1.75/L from scratch (i.e., without providing v1 of the
paper in context, but simply asking the main question studied here directly).

5

Theorem I.1.1 (Improved step-size condition). Let f : Rd → R be convex and L-
smooth, and consider gradient descent

xk+1 = xk − η∇f (xk),

k ≥ 0.

If η ∈ (0, 3

2L ], then the sequence {f (xk)}k≥0 is convex, in the sense that the decreases

form a nonincreasing sequence: Dk+1 ≤ Dk for all k ≥ 0.

Dk := f (xk) − f (xk+1)

Proof. Write gk := ∇f (xk) and ∆k := gk+1 − gk. We compare two consecutive
decreases Dk and Dk+1.
(1) A lower bound for Dk. For convex L-smooth f , the Bregman divergence obeys

1
2L

∥∇f (x) − ∇f (y)∥2 ≤ f (x) − f (y) − ⟨∇f (y), x − y⟩.

With x = xk, y = xk+1 and xk − xk+1 = ηgk this gives

Dk ≥ η ⟨gk+1, gk⟩ +

1
2L

∥∆k∥2.

(A)

(2) An upper bound for Dk+1. By convexity, f (x) − f (y) ≤ ⟨∇f (x), x − y⟩. With
x = xk+1, y = xk+2 and xk+1 − xk+2 = ηgk+1,

Dk+1 ≤ η ∥gk+1∥2.

(B)

(3) Subtract and use cocoercivity. Subtracting (B) from (A) yields

Dk − Dk+1 ≥ η ⟨gk+1, gk − gk+1⟩ +

1
2L

∥∆k∥2 = −η ⟨gk+1, ∆k⟩ +

1
2L

∥∆k∥2.

Since gk+1 = gk + ∆k, we have ⟨gk+1, ∆k⟩ = ⟨gk, ∆k⟩ + ∥∆k∥2, hence

Dk − Dk+1 ≥ −η ⟨gk, ∆k⟩ +

(cid:16) 1
2L

(cid:17)

− η

∥∆k∥2.

For convex L-smooth f , gradients are 1/L-cocoercive:

∥∆k∥2 ≤ ⟨∆k, xk+1 − xk⟩ = ⟨∆k, −ηgk⟩ = −η ⟨gk, ∆k⟩.

1
L

Therefore,

Dk − Dk+1 ≥

(cid:16) 1
L

+

1
2L

(cid:17)

− η

∥∆k∥2 =

(cid:16) 3
2L

(cid:17)

− η

∥∆k∥2.

If η ≤ 3
equivalent to the convexity of the sequence {f (xk)}k≥0.

2L , the right-hand side is nonnegative, so Dk+1 ≤ Dk for all k, which is
□

Figure I.2: GPT-5 Pro’s proof of the sufficient condition η ≤ 1.5/L.

6

I.2 Discovering new black hole symmetries with GPT-5 – Alex

Lupsasca

We document, in a standardized format, a theoretical calculation in black hole physics
performed by an AI. GPT-5 Pro (re)derived nontrivial Lie point symmetries—including an
SL(2, R) algebra—of the stationary, axisymmetric wave equation on a Kerr background. The
model initially failed on the curved-space problem, but then succeeded after a flat-space
warm-up, ultimately producing the correct symmetry generators that underpin the recent
results in [Lup25b] (to which the model did not have access).

I.2.1 The problem in context

Astrophysical black holes are characterized by their mass M and angular momentum J = aM .
These two quantities completely determine the black hole spacetime geometry: it is described
by the Kerr metric with parameters (M, a).

We study massless, stationary, axisymmetric waves on a rotating (Kerr) black hole. In
Boyer-Lindquist coordinates (t, r, x, ϕ) with x = cos θ, the governing equation is the scalar
Laplace operator restricted to these symmetries,

∂r[∆(r) ∂rψ(r, x)] + ∂x

i
h
(1 − x2) ∂xψ(r, x)

= 0,

∆(r) = r2 − 2M r + a2.

(I.1)

This is a linear second-order partial differential equation in two variables. Its solutions encode
the static tidal response of black holes; their asymptotics determine horizon deformability
(“Love numbers”). In general relativity, black holes exhibit vanishing (static) Love numbers, a
surprising rigidity often traced to hidden symmetries of the field equations.

A key question is thus: what are the symmetries of Eq. (I.1)?

Human route. Classically, one would apply Lie’s algorithm for reduction of order to
Eq. (I.1). In this procedure, one first posits a symmetry generator X = ξr ∂r + ξx ∂x + η ∂ψ,
prolongs X to derivatives, then imposes invariance of Eq. (I.1), and finally solves the resulting
overdetermined system for the unknown functionss (ξr, ξx, η). This is arduous in a curved black
hole background (coefficients depend on r and x via ∆ and 1 − x2). In recent work [Lup25b],
I carried out this analysis and was surprised to find three nontrivial Lie point symmetries
that generate an SL(2, R) algebra. These symmetries include a dilatation H0 associated with
a scale invariance that can be used to explain the absence of tidal response (vanishing Love
numbers) for black holes in general relativity.

Additional references. For background on Kerr and wave equations:
[Ker63; Teu73;
Wal84]. For black-hole tidal response and Love numbers, our main technical reference is
[Lup25b] and citations therein.

I.2.2 The task given to GPT-5 Pro: goal, prompts, and model behavior

Goal. Elicit from GPT-5 Pro the Lie point symmetries of (I.1), ideally reproducing the
SL(2, R) generators and their curved-space coefficients recently obtained in [Lup25b].

Outcome. After ∼5 minutes of internal reasoning, the model (incorrectly) reported that
the equation had no continuous symmetries beyond trivial scalings.

7

What are the Lie point symmetries of the PDE (I.1)?

[Reasoned for 5m 13s]
[Incorrect answer: GPT-5 Pro finds no symmetries and wrongly asserts that none
exist.]

Warm-up prompt and outcome (success on flat space). To scaffold the task, we
initialized a new instance of the model and first posed the flat-space limit M, a → 0 in
cylindrical coordinates (ρ, z) with ρ = r

1 − x2, z = rx:

√

(cid:16)

ρ + ∂2
∂2

z +

(cid:17)

∂ρ

1
ρ

ψ(ρ, z) = 0.

(I.2)

Outcome. After 10 minutes and 27 seconds, the model produced all the symmetries,
including three that generate SL(2, R):

H+ = ∂z,

H0 = ρ ∂ρ + z ∂z +

1
2

,

H− = 2ρz ∂ρ − (ρ2 − z2) ∂z + z.

(I.3)

Here H− is the nontrivial special conformal generator; obtaining it suggests that the model
executed (or emulated) a real symmetry computation rather than guessing.

What are the Lie point symmetries of (I.2)?

[Reasoned for 10m 27s]
[Correct answer including generators (I.3).]

What are the Lie point symmetries of (I.1)?

[Reasoned for 18m 9s]
[Correct answer including generators (I.4).]

Second curved-space attempt and outcome (success). We then gave the same instance
of GPT-5 the same prompt as before: “What are the Lie point symmetries of (I.1)?”

8

Outcome. Within ∼18 minutes, the model produced the correct curved-space generators
closing into SL(2, R):

H+ =

x ∆ ∂r + (r − M )(1 − x2) ∂x
(r − M )2 − (M 2 − a2)x2

,

H0 =

(r − M ) ∆ ∂r + (M 2 − a2)x(1 − x2) ∂x
(r − M )2 − (M 2 − a2)x2

+

1
2

,

(I.4a)

(I.4b)

H− =

(M 2 − a2)x ∆ ∂r − (r − M )(1 − x2)(cid:2)∆ − (M 2 − a2)x2(cid:3) ∂x
(r − M )2 − (M 2 − a2)x2

+ x ∆ ∂r + (r − M ) x.

(I.4c)

Analysis.

In summary, here is

• What GPT-5 got right: The relatively simple flat-space symmetries; the full nontrivial

curved-space coefficients; the SL(2, R) structure.

• What GPT-5 got wrong (along the way). The cold start on Eq. (I.1) incorrectly
concluded “no symmetries.” The model appears to have needed to “warm up” via the
simpler flat-space problem (I.2) sharing the same symmetry structure.

Log. For verification and reproducibility purposes, this conversation with GPT-5 can be
accessed here: [Lup25a].

I.2.3 Result, implications, and next steps

Result. GPT-5 Pro (re)discovered the curved-space SL(2, R) symmetry generators (I.4) of
Eq. (I.1). This matches the key structural insight of [Lup25b]. Practically, once the symmetry
is known, downstream results (e.g., constraints on tidal response and the vanishing of static
Love numbers in this sector) follow with comparatively modest analysis.

Reflection on the interaction. Two observations:

1. Scaffolding mattered. The model failed “cold” but succeeded rapidly after a closely
related warm-up. This suggests retrieval or internal pattern activation can be primed by
presenting a simpler member of the same symmetry class.

2. Algorithmic plausibility. The final generators are too structured to be a lucky guess.
The model likely executed (implicitly) a mix of: recognizing conformal invariance in the
flat equation, hypothesizing a curved analogue, and/or exploiting a coordinate map that
simplifies Eq. (I.1) toward Eq. (I.2).

Implications

• AI as a symmetry engine. With minimal domain scaffolding, current models can
carry out nontrivial Lie-symmetry discovery for PDEs with non-constant coefficients.

• Research velocity. Given such capabilities, the time from idea to publishable result
can compress from months to days once the right prompts and scaffolds are in place.

9

• Generalization opportunity. The same workflow (warm-up on simplified problems,
then lift) can be applied to more complex problems of physical interest in black hole
theory and beyond.

Takeaway. GPT-5 Pro, when properly scaffolded, uncovered the SL(2, R) symmetry content
of a curved-space PDE central to black-hole tidal response. This supports a broader thesis:
contemporary LLMs can act as practical assistants for symmetry discovery and analytic
structure mining in theoretical physics.

10

I.3 Mechanistic analysis and outcome prediction for in vitro
immune system experiments using GPT-5 Pro – Derya
Unutmaz, M.D.

Here, I demonstrate that GPT-5 Pro successfully analyzed a figure from an experiment with
human T cells cultured with 2-deoxy-D-glucose (2-DG) that showed an increased proinflam-
matory Th17 cell subset. This experiment was performed in our lab several years ago, but the
mechanism remained unclear. GPT-5 Pro provided the key mechanism that could explain
these findings and, in addition, made highly relevant experimental suggestions. The mecha-
nistic insight and further hypothesis to dissect these findings were highly valuable and not
immediately obvious, despite our deep expertise in this field. In a subsequent unpublished
figure, GPT-5 Pro interpreted flow cytometry data of the checkpoint inhibitors PD-1 and
LAG-3 on cytotoxic T cells after transient glycolysis inhibition, inferring that 2-DG was
reprogramming inhibitory receptor expression through combined effects on glycosylation and
through attenuated T cell receptor signaling. It further correctly predicted that a brief 2-DG
pulse during CAR-T cell generation from these cells would enhance their cytotoxicity towards
target cancer cell lines, which we had internally validated in unpublished results. Together,
these examples illustrate how GPT-5 Pro can function as a true mechanistic co-investigator in
biomedical research, compressing months of reasoning into minutes, uncovering non-obvious
hypotheses, and directly shaping experimentally testable strategies.

I.3.1 Problem Context

We had been studying how subsets of human immune cells called T cells respond to mod-
ifications in their glucose metabolism. There was significant evidence that glucose uptake
and energy metabolism influence the differentiation of T cells into effector subsets, which can
enhance protection against infections or drive pathologies such as chronic inflammation and
autoimmunity, and could have implications during cancer immunotherapy. To dissect this
mechanism, we used a competitive analog of glucose, 2-deoxy-D-glucose (2-DG), as an inhibitor
of glucose uptake in T cells. Because T cells are highly heterogeneous, we first sorted them
into CD4+ and CD8+ subsets, which are helper and cytotoxic T cells, respectively. In the
initial experiment, we used CD4+ T cells and activated them in the presence of varying doses
of 2-DG or under low-glucose conditions. After two days, these treatments were washed out to
allow the T cells to proliferate in the presence of interleukin-2 (IL-2), a growth-factor cytokine.
The idea was that any impact of glucose inhibition would occur early in the experiment,
become imprinted during priming, and, after two weeks of expansion, manifest as differences
in T-cell differentiation and function.

I.3.2 The task given to GPT-5: goal, prompts, and model behavior.

The figure in the prompt below is derived from flow cytometry data on cells cultured with
2-DG. I uploaded it as part of the prompt, which led to the following conversation with GPT-5.
While we had published a similar findings as a preprint, this specific figure or experiment had
not been previously published.

11

Context and question: We tested how transient glycolysis inhibition
during human T cell priming affects later Th17 function. Sorted
CD4+ T cells were stimulated with anti-CD3 and anti-CD28 in the
presence of specified concentrations of 2-deoxy-D-glucose (2-DG).
Cells were then expanded for two weeks without 2-DG, re-stimulated,
and assayed for IL-17A together with CCR6 or CD161. Analyze
the provided flow cytometry dataset obtained after stimulating
these two-week cultured cells with PMA plus ionomycin for 6 hours,
followed by intracellular IL-17A staining and surface staining for
CCR6 and CD161. Explain the results, derive mechanisms, extract
high-level conclusions, assess impact, and recommend follow-up
experiments. Also state how interpretations would differ if the same
design started from sorted naive CD4+ T cells instead of memory
cells.

• A brief executive summary of the main findings.

• A quantitative readout of dose responses for IL-17A, CCR6,
and CD161, including percent positive, median fluorescence
intensity, and effect sizes with confidence intervals across
donors.

• Clear plots or tables comparing 2-DG doses to control, both at
priming and after the two-week washout, indicating whether
effects persist after removal of 2-DG.

• An interpretation of how 2-DG altered Th17 biology, consid-

ering all potential mechanisms.

• High-level conclusions and potential impact for immunother-

apy, autoimmunity, and other immune responses.

• A concise plan for follow-up experiments that will help decode
the potential mechanisms and identify the precise mechanism.

• A comparison of expected outcomes if the experiment started
with naive CD4+ T cells versus memory CD4+ T cells, rather
than total CD4+ T cells that contain a mixture of naive and
memory subsets, including suggested follow-up experiments
to resolve mechanism.

• Key follow-up experiments to establish mechanism and causal-

ity, with potential outcomes.

12

[Reasoned for 17m]

Briefly exposing human CD4+ T cells to 2DG during the first activation only
leaves a durable, dose-dependent imprint detectable two weeks later: the frequencies of
IL-17A+, CCR6+, and CD161+ cells all rise, with the largest increases at 3mM 2DG.
Marker increases (CCR6, CD161) are substantially larger than the increase in IL-17A
itself, implying an expansion or imprinting of Th17-lineage memory CCR6+/CD161+)
that is poised for, but not uniformly executing, IL-17A production. A lowglucose
control (0.5 mM) partly mimics the effect but is weaker than 2DG at equal nominal
concentrations, suggesting contributions from glycolysis restriction and N-linked
glycosylation interference (a known 2DG off-target) rather than energy limitation
alone. The effect persists after drug washout, indicating cellular reprogramming,
selective survival/expansion of Th17-lineage cells, or both.

After 17 minutes of reasoning, GPT-5 Pro produced multiple histogram figures like these
two examples, that perfectly analyzed the data, along with an outstanding executive summary
that captured the key mechanistic points.

It also generated a quantitative dose–response readout for the figure (output not shown)
and a CSV file containing all underlying data used to generate the histograms. GPT-5 Pro
then provided an interpretation of how 2-DG treatment altered Th17 biology. The mechanistic
inferences from GPT-5 Pro were pivotal, and some were ideas we had not previously considered.
In retrospect, in particular, the proposed mechanism of reduced IL-2 signaling via interference
with N-linked glycosylation made clear biological sense, because it could directly explain the
disinhibition of Th17 cell differentiation under 2-DG treatment. However, this mechanistic
hypothesis had not occurred to us previously, as making this connection was not immediately
obvious. Other mechanistic hypotheses were also highly relevant and plausible.

13

What the data say:

A single 2DG exposure at priming led, two weeks later, to (i) a 13× increase
in IL-17A+ cells, (ii) a 16–20× increase in Th17-lineage markers (CCR6, CD161), and
(iii) disproportionately more CCR6+/CD161+ cells than IL-17A+ cells. This pattern
fits an enrichment and/or imprinting of Th17 lineage memory rather than a
uniform gain in acute effector cytokine output across all cells. Many CCR6+/CD161+
cells are IL-17A− at re-stimulation, a hallmark of poised Th17 memory that can
produce IL-17A under the right cues (for example, IL-1β + IL-23).

Mechanistic inferences.
this durable skew:

Several nonexclusive mechanisms could generate

1. Reduced IL2 signaling via N-linked glycosylation interference. 2-DG
at mM levels perturbs Nglycan synthesis, which in activated T cells reduces
proper surface expression of glycoproteins—including IL2Rα (CD25). Diminished
IL2/STAT5 signaling disinhibits Th17 differentiation and limits conversion of
Th17 cells to Th1-like states. The stronger effect of 2DG than low glucose supports
a role for glycosylation beyond mere ATP/pyruvate restriction.

2. Acute metabolic checkpointing that locks in a lineage program. Transient
glycolytic inhibition can shift activation metabolism toward mitochondrial oxi-
dation, altering mTORC1/HIF1α activity and chromatin modifiers (acetylCoA,
α-ketoglutarate), leaving an epigenetic memory at Th17-relevant loci (RORC,
IL23R, CCR6). The persistence after washout is most consistent with such a
program.

3. Selective survival/expansion. 2DG may preferentially limit proliferation
or survival of non-Th17 subsets (e.g., Th1), effectively enriching preexisting
CCR6+/CD161+ memory present in the sorted total CD4+ pool. The larger
fold increase in CD161/CCR6 than in IL-17A favors this explanation as at least a
contributor.

4. Altered TCR/CD28 signal strength. Perturbed glycosylation also modulates
TCR and co-stimulatory receptor function during priming. Weaker IL-2-rich
effector programs and preserved RORγt/AHR programs would tilt toward the
Th17 lineage.

High level conclusions and potential impact: A onetime metabolic perturbation
during priming imprints a durable Th17-leaning memory phenotype in human CD4+ T
cells. Because Th17/Tc17 states can display superior persistence and tissue trafficking,
this imprint could be useful for adoptive T cell therapies (including CAR-T) when a
Th17-like program is desired. Conversely, it warns that transient glycolysis inhibition—
whether intentional (e.g., 2-DG as an anti-metabolite) or incidental (nutrient limitation
in bioreactors)—might increase Th17-lineage memory and thereby exacerbate Th17-
mediated autoimmunity if applied in vivo. The clear persistence after washout argues
we are not seeing acute metabolic suppression but rather a biologically meaningful
reprogramming.

GPT-5 Pro then proposed key follow-up experiments. We had already performed the first
of these experiments, the mannose rescue experiment, weeks after the first experiment. The

14

result matched GPT-5 Pro’s prediction exactly. However, although we have not published
this specific figure, we did publish a preprint in BioRxiv that included the mannose rescue
experiment in a similar story. Therefore, while impressive, a caveat for this suggestion is that
GPT-5 Pro may have known about this finding and made the connection with this result.

That said, GPT-5 Pro also suggested additional, even more important experiments targeting
N-linked glycosylation using inhibitors, as well as experiments to interrogate the IL-2 axis
during priming, neither of which we have performed nor published in the context of this
specific finding. We are also not aware of any other publication performing these experiments
for Th17 cell differentiation status after 2-DG treatment. In addition, the suggestions by
GPT-5 Pro to map the functional plasticity of Th17 cells and their epigenetic imprinting are
striking, because they require an extremely deep understanding of Th17 cell biology.

To discriminate among mechanisms and establish causality, the most informative next
steps are: Partition “glycolysis block” vs “glycosylation block.” Repeat the
priming step with:

1. 2-DG±mannose rescue (restores Nglycosylation without restoring glycolysis)

2. specific glycolysis inhibitors that do not inhibit Nglycosylation (e.g.,
PFKFB3 inhibitor 3PO or iodoacetate at carefully titrated, nontoxic
doses), and

3. Nglycosylation inhibitors (e.g., tunicamycin) at low doses.

If mannose
reverses the 2-DG effect while glycolysis-only inhibitors do not phenocopy it,
glycosylation is the driver.

Interrogate the IL2 axis during priming. Quantify surface CD25 maturation
(PNGase F shift), pSTAT5, and IL2 consumption. Rescue experiments with exoge-
nous IL2, or IL2/antiIL2 immune complexes that enforce STAT5 signaling, should
counteract Th17 bias if IL2 pathway suppression is causal.

Track proliferation/survival and composition. Use CellTrace/Ki67 and Annexin
V at 24–96 h after priming to determine whether 2-DG is enriching CCR6+/CD161+
cells through selective growth/survival. Include absolute cell counts so percentages
can be interpreted.

Define the epigenetic imprint. Perform ATACseq and targeted CUT&Tag for
H3K27ac/H3K4me3 at RORC, IL-17A/F, IL23R, CCR6, and PRDM1 after priming
±2-DG and again at day 14. Lasting chromatin accessibility gains would confirm a
programmed memory.

Map functional plasticity. After the two-week washout, restimulate with IL-1β +
IL-23 (Th17 stabilizing) or IL12 (Th1 skewing) to see whether 2DG-primed cells are
preferentially locked into or resistant to Th1 conversion.

Rule out MAIT contamination. Because CD161 is also expressed by MAIT cells,
add TRAV12 (Vα7.2) and MR1tetramer staining. This will confirm the cells are bona
fide Th17-lineage CD4+ T cells.

Metabolism readouts. Seahorse ECAR/OCR at 24–48h post-priming to verify
the intended glycolytic restriction and compensatory mitochondrial changes; measure
mTORC1 readouts (pS6) and HIF1α.”

15

In the follow-up prompt, I also uploaded the results of the mannose rescue experiment we
had published. GPT-5 Pro confirmed previous suggestions and further refined the mechanistic
model to explain these results together with the similar follow-up experiments suggested
(output not shown).

Integrating both datasets and the naïve vs memory result
Your original titration showed a monotonic 2-DG dose–response and persistent increases
in CCR6/CD161 well after washout; your new experiment shows that mannose fully
normalizes or nearly normalizes these increases. Together with your subset sort, three
strong inferences follow.

1. The 2-DG imprint is driven primarily by impaired N-linked glycosylation during
priming. Mannose bypasses the 2-DG block in the dolichol-linked oligosaccharide
pathway and rescues normal glycoprotein maturation. The near-complete rescue at
3mM mannose means glycolysis inhibition is not the principal cause of the Th17
skew.

2. The responding cells are memory, not naïve, CD4+ T cells. The absence of effect
in naïve cells rules out a simple “2DG instructs naïve Th17 differentiation” model
under your conditions. Memory cells likely contain preexisting CCR6+/CD161+
Th17-lineage clones; 2-DG preserves or expands them during priming by attenuating
IL-2/STAT5 and/or by dampening TCR/CD28 glycoprotein function, both of which
otherwise push cells away from Th17 toward Th1/CTL-like fates.

3. Programming plus selection best explains the magnitude and durability. Short
term glycosylation stress at priming can both select for Th17-lineage memory
cells (differential proliferation/survival) and program chromatin at Th17 loci. The
persistence after two weeks without drug argues that at least part of the effect is
epigenetically stabilized.

Mechanistic model that fits all observations. During the first 24–48h of ac-
tivation, 3mM 2DG impairs N-glycosylation of multiple surface receptors (notably
IL2Rα/CD25, TCR, CD28, potentially IL-6R and IL-23R). Hypoglycosylated receptors
are retained or function less efficiently at the membrane, which reduces IL-2/STAT5
signaling and blunts strong co-stimulation. Because IL-2/STAT5 antagonizes Th17
and promotes Th1/Treg fates, this temporary reduction biases activated memory
cells toward retaining or consolidating a Th17 program. Mannose restores Nglycan
synthesis, normalizing receptor glycosylation and thereby reversing the imprint. Naïve
cells, which depend heavily on IL-2 for survival and lack precommitted Th17 chromatin,
do not show the same skew; any reduction in IL-2 signaling at priming in naïve cells
tends to reduce expansion rather than drive Th17 features.

This model predicts that, in 2-DG-primed memory cultures: surface CD25 will show
lower expression and altered glycoform size; pSTAT5 will be reduced; ATACseq will
show enhanced accessibility at RORC, IL-23R, CCR6; and proliferation/survival curves
will show relative sparing or enrichment of CCR6+/CD161+ cells.

This final section on impact, implications, and high-level interpretation offered particularly

remarkable insight into these hypothesis and proposed experiments.

16

Because naïve cells did not respond, the 2DG effect does not instruct de novo Th17
programming from naïve precursors under these conditions. Instead, it selects and
stabilizes Th17-lineage features in memory cells already carrying partial RORC/IL-
23R/CCR6 programs. This distinction has practical consequences. If your aim is to
manufacture Th17-leaning T cell products for therapy, enriching memory CD4+ (and
potentially CD8+) compartments before a short 2-DG priming pulse will be far more
effective than applying 2-DG to naïve enriched starting material. Conversely, if you
are attempting to avoid Th17 skew in a culture process, maintaining normal glycosyla-
tion (adequate mannose/glucose; avoiding 2-DG exposure during early activation) is
essential.

Impact and applications. For immunotherapy, a brief, controlled glycosylation
perturbation at priming could be used to generate Th17/Tc17-leaning memory with
improved persistence and trafficking, then returned to normal glycosylation by washout
or mannose supplementation. For autoimmunity, the data warn that 2-DG –or any
intervention that transiently depresses N-glycosylation during T cell activation—could
exacerbate Th17 immunity, especially within existing memory pools. For vaccine
design, manipulating glycosylation during priming may tune helper subset balance
without permanent genetic changes.”

I then uploaded an unpublished flow cytometry figure showing the effect of 2-DG on the
checkpoint inhibitors PD-1 and LAG-3 on activated CD8+ naive and memory T cells (Figure
I.3 shown below) with similar prompt to the original. GPT-5 Pro perfectly analyzed and
described the data in the figure:

A brief, transient exposure to 3 mM 2-DG during anti-CD3/CD28 activation profoundly
remodels early checkpoint receptor expression on human CD8+ T cells measured on
day 4. In naïve CD8+ cells, 2-DG collapses the PD-1+ LAG-3+ population from
∼ 94% control to ∼ 0.8%, leaving most cells double-negative. In memory CD8+ cells,
2-DG reduces LAG-3 strongly (from 92 percent to ∼ 57% LAG-3+) and decreases
the double-positive subset to ∼ 55%, with total PD-1+ only modestly reduced (from
∼ 99% to ∼ 89%). In glucose-free media (glycolysis restriction without glycosylation
block), naïve cells show only a partial reduction of LAG-3 (to ∼ 38%) and little change
in PD-1; memory cells are largely unchanged. Thus, 2-DG’s effects are far stronger
than glucose deprivation and are consistent with impaired N-linked glycosylation
rather than energy shortage. The pattern dovetails with your earlier findings in CD4+
T cells: a brief, early glycosylation perturbation leaves a durable imprint that shifts
fate and checkpoint expression.

In the follow-up, GPT-5 Pro provided another key mechanistic interpretation of why LAG-3
and PD-1 could be lower on CD8+ T cells due to glycosylation impairment, given that both are
heavily N-glycosylated. It also provided a second and very insightful mechanistic explanation
in which reduced signal strength could attenuate TCR signaling during 2-DG exposure during
the activation phased of these cells and thus this modulation in signaling could have prevented

17

Figure I.3: Naive and memory CD8+ T cells were activated with anti-CD3/CD28 beads
in media alone, 3mM 2DG, or in glucose-free media.Representative flow cytometry plots of
PD1 and LAG3 surface expression on day 4- post activated naive (top) and memory T cells
(bottom) in indicated conditions were shown.

upregulation of PD-1 and LAG-3, especially in naive T cells. Remarkably, we have direct and
unpublished experiments showing this exact reduction in signal strength, which is remarkably
much more pronounced in naive T cells compared to memory subset. Both PD-1 and LAG-3
are key checkpoint inhibitors that dampen responses against cancer cells. Indeed, the major
cancer immunotherapy treatments target PD-1 or its ligand to enhance the responses of CD8+
T cells toward cancer cells.

Mechanistic interpretation. 2-DG is a glucose analog that both limits glycolysis
and interferes with N-linked glycosylation, a post-translational modification required
for correct folding, trafficking, and surface stability of many receptors. LAG-3 and
PD-1 are heavily N-glycosylated. The drastic, mannose-rescuable effects you already
demonstrated in CD4+) cells, combined with the present observation that no-glucose
fails to phenocopy 2-DG, point to glycosylation impairment as the dominant mechanism
here. Two layers likely operate:

1. Direct receptor effect. Hypoglycosylation reduces proper surface expression and
stability of LAG-3 (and to a lesser extent PD-1, especially in naïve cells), rapidly
lowering measured positivity.

2. Signal-strength effect. By hypoglycosylating TCR/CD28 and IL-2Ralpha (CD25)
during priming, 2-DG attenuates early TCR/co-stimulatory and IL-2/STAT5 signals
that otherwise drive robust PD-1/LAG-3 induction. This is more evident in naïve cells,
which rely on strong primary signals; memory cells, with lower activation thresholds,
still upregulate PD-1 but lose much of their LAG-3.

18

High-level conclusions and impact
A short, early 2-DG pulse is a powerful dial that reduces inhibitory checkpoint display
(especially LAG-3) on CD8+) T cells, with a stronger effect in naïve than in memory
cells. The biology is glycosylation-dependent rather than energy-deficit-dependent.
Because PD-1/LAG-3 co-expression correlates with early dysfunction/exhaustion,
transient 2-DG during priming may produce CD8+ products that are less prone to
early exhaustion and better poised for sustained activity after drug washout.

Immunotherapy implications. For CAR-T or TCR-engineered products derived from
memory CD8+ cells, a brief 2-DG exposure during the first 24–48 hour could generate
cells with lower LAG-3 and slightly lower PD-1, potentially enhancing resistance to
exhaustion under antigen pressure and improving serial killing over days. This matches
your CD4+) memory dataset, where early 2-DG favored type-17-like, memory-leaning
states that persist after washout. Conversely, for autoimmunity the data caution
that interfering with glycosylation during priming could produce effector-competent,
checkpoint-low CD8+) responses that exacerbate pathology.

Importantly, I asked GPT-5 Pro to simulate an experiment in which we would genetically
engineer these CD8+ T cells with anti-CD19 CAR molecules to target CD19+ lymphomas,
and to predict how prior 2-DG treatment would impact the cytotoxicity of these cells toward
the cancer cells. Because we had already performed this experiment, also unpublished, I
was able to validate the prediction from GPT-5 Pro, and it was nearly perfect in predicting
that cytotoxicity would be enhanced. It also proposed mechanistic insights and follow-up
experiments that we would need to perform before publishing these results. In this regard,
GPT-5 Pro made sufficient contributions to this work to the extent that it would warrant its
inclusion as a co-author in this new study.

What this predicts for anti-CD19 CAR memory CD8+ cells (simulation)
Memory CD8+) cells are activated, transduced with an anti-CD19 CAR, expanded for
2 weeks in IL-2, washed free of drug, then co-cultured with CD19+ lymphoma cells
for 3 days. Outcomes are read as specific lysis and serial-killing capacity. To isolate
functional differences, analyses gate on CAR+ cells, to control for any 2-DG effects
on transduction efficiency. Mechanistic expectations applied to CAR-T context. The
2-DG pulse yields lower LAG-3 and modestly lower PD-1 on day 4, which typically
translates to less activation-induced dysfunction over repeated target encounters.
Although acute glycolysis supports immediate cytotoxic degranulation, your exposure
is transient and occurs two weeks before the assay; metabolic capacity should recover,
while the checkpoint/lineage program imprinted early is retained. Therefore, initial
killing at 24 h may be similar or slightly lower, but cumulative 72 h killing should be
higher with 2-DG priming, especially at low E:T ratios.

I.3.3 Key takeaways and implications from this case study of GPT-5 Pro

In summary, GPT-5 Pro provided remarkable key insights and hypothesis based on our
experimental data. If we had had these interpretations and the recommended next experimental
plan from GPT-5 Pro, we would have resolved or hypothesized the mechanistic insights within

19

minutes upon data analysis. Importantly, we still missed key insights provided by GPT-5 Pro,
despite our deep expertise of this field. In addition, GPT-5 Pro suggested several experiments
that we will need to perform to publish these findings, that’s a co-investigator level contribution.
More specific takeaways and implications are:

1. Precision interpretation of complex biology. GPT-5 Pro rapidly connected the
observed phenotypes to a mechanistic hypothesis that distinguished glycolysis blockade
from impaired N-linked glycosylation, proposed the IL-2 receptor pathway as the driver,
and predicted that memory rather than naïve T cells carry the effect.

2. Testable next steps that held up in the lab. GPT-5 Pro produced a clean decision
tree of experiments, including the mannose rescue to restore glycosylation, metabolic
readouts, and epigenetic assays. This shows the model can generate highly relevant
hypotheses that are testable in wet-labs.

3. Mechanism-first thinking that avoids false trails. By separating selection effects
from programming effects, and by proposing controls that disentangle glycosylation from
energy restriction, GPT-5 Pro reduced the risk of chasing attractive but potentially
unnecessary experiments, which would have wasted many months of testing.

4. AI-guided bioengineering for cell therapies. By correctly predicting that a brief 2-
DG exposure during priming would lower PD-1/LAG-3, preserve cytotoxic potential, and
enhance serial killing in anti-CD19 CAR memory CD8+ T cells, GPT-5 Pro illustrates
how foundation models can propose concrete, testable tweaks to CAR-T cell development
protocols that enhance therapeutic performance against cancer or autoimmune diseases
by rapidly iterating a variety of conditions in silico, before wet-lab validation.

5. Long-term impact on biomedicine. As models like GPT-5 Pro become native to lab
operating systems, we should expect: faster mechanism discovery across immunology,
oncology, and metabolism; cheaper negative results because failed branches are pruned
in silico; more reproducible science due to selecting better hypothesis and well desisgned
experimental approaches. The net effect will be a much higher discovery rate per
experiment and a shorter route from observation to discovery to intervention, thus
profoundly accelerating the biomedical scientific process.

20

Chapter II

Deep literature search

II.1 From density estimation and convex geometry to multi-

objective optimization – Nikita Zhivotovskiy

The experiment discussed here originally dates to late August 2025, before a number of
subsequent applications circulated more broadly. The goal was to apply GPT-5 not to obtain
a direct literature answer, but to see whether a newly proved geometric statement would
immediately surface adjacent literature and applications. This was useful to the authors
because plausible extensions were unclear and would otherwise require asking a number of
experts (and luck) to find the right connections.

Our preliminary conclusion from this experiment is that, given only a core mathematical
statement, GPT-5 can rapidly surface nontrivial and technically aligned links across areas
(here, multiobjective optimization and approximate Pareto sets), providing context for new
applications.

First, we record the statement that motivated the experiment. Let K be a compact
subset of Rd
+ and α ≥ 1. We say that a subset A ⊂ K is an α-ratio cover of K if for every
θ = (θ1, . . . , θd) ∈ K there exists ϕ = (ϕ1, . . . , ϕd) ∈ A such that θj ≤ α ϕj for all j = 1, . . . , d.

Theorem II.1.1 (Compton, Lugosi, Mourtada, Qian, Zhivotovskiy, 2025). For every d ≥ 1
and every convex, compact set K ⊂ Rd
+, there exists a subset A ⊂ K with at most 28d elements
that is a 32-ratio cover of K.

Note that GPT-5 was not involved in the proof of the result above. The original motivation
for Theorem II.1.1 is the statistical problem of density estimation for mixtures. Let {f1, . . . , fd}
be densities on Rp and, for mixing weights θ ∈ K ⊂ ∆d−1 with ∆d−1 = {θ ∈ Rd
i=1 θi = 1},
consider the mixture pθ(x) = Pd
i=1 θifi(x). If A is an α-ratio cover of K, then for any θ ∈ K
there exists ϕ ∈ A with θ ≤ α ϕ coordinatewise; consequently, pθ(x) ≤ α pϕ(x) for all x, hence
pθ(x)
pϕ(x) ≤ α pointwise and, in particular, KL(pθ∥pϕ) ≤ log α. This helps with KL-based covering
bounds for {pθ : θ ∈ K}.

+ : Pd

When obtaining a result of the form of Theorem II.1.1, one naturally asks whether it
is known and where it applies. What GPT-5 was helpful to do, once the authors had
Theorem II.1.1, was to surface related results that broaden the scope into theoretical computer
science. To keep the experiment simple and clean:

1. the work [Com+25] was not public at that time, so its statements and possible connections

were not available to the model;

21

2. for this note, the queries were rerun in a temporary, new session to avoid contamination

from prior searches/interactions.

[Theorem II.1.1 and the preceding paragraph are put in context.]

What are the similar known quantitative results? List the most
relevant results in different areas (geometry, statistics). Make a
short list with important/central papers for references/topics

[Reasoned for 8m 4s]

[...]
Your “α-ratio cover” is essentially the (multiplicative) ε-approximate Pareto set notion
used in multicriteria optimization (dominance up to a (1 + ε)-factor in each coordinate).

• Papadimitriou–Yannakakis (FOCS 2000). Introduced the modern framework:
conditions under which a polynomial-size (1 + ε)-approximate Pareto set exists
and can be computed; foundational for multiplicative coordinate-wise coverage.

...

Figure II.1: An excerpt from GPT-5’s reply. The full response can be found in [Zhi25].

Figure II.2: A result of [PY00] closely related to Theorem II.1.1.

As can be seen in Figure II.1, among a series of other relevant results, the August 2025
exchange prominently highlighted the seminal work of [PY00] on multiobjective optimization.
At first sight (see Figure II.2), this seems unrelated to Theorem II.1.1 and could be mistaken
for a hallucination. However, unpacking their proof shows that their result can be phrased as a

22

coordinatewise (1+ε)-ratio cover on K ⊂ [1/R, R]d of size O
. In contrast, under
convexity of K, Theorem II.1.1 yields a constant factor 32 and a size bound 28d independent
of R. In the Papadimitriou–Yannakakis framework, K denotes the set of achievable objective
vectors; convexity is not assumed in general (though convex instances do arise).

(log R/ε)d−1(cid:17)
(cid:16)

A substantial part of [PY00] is devoted to constructing near-optimal ε-approximate Pareto
sets. Motivated by this connection, we extended Theorem II.1.1 to (1 + ε)-covers of size
O (cid:0)(log(1/ε)/ε)d−1(cid:1) that, for convex K, are independent of the range parameter R. Conse-
quently, in convex instances this removes the log R factor from the classical O (cid:0)(log R/ε)d−1(cid:1)
bounds and provides a benchmark for convex multiobjective optimization.

23

II.2 Erdős problems (part 1/2) – Mehtaab Sawhney and Mark

Sellke

II.2.1

Introduction

Paul Erdős was a prolific mathematician whose mathematical impact ranges across diverse areas
of mathematics including combinatorics, number theory and analysis. Along with publishing
more than 1500 mathematical papers, Erdős posed a substantial number of mathematical
conjectures with several of these conjectures becoming central problems in mathematics.

Recently Thomas Bloom [Blo] created an online index of Erdős problems, which has
greatly facilitated the process of identifying and solving the problems and opened the door
to numerous online collaborations. The role of large language models in our story already
begins here with the creation of the website in early 2024, which relied on a previous version
of ChatGPT (https://www.erdosproblems.com/faq) to help write the code underlying the
website. Each listed problem has its own webpage, with a status (roughly speaking, either
“open” or “solved”) as well as a discussion thread for potential collaborations. The list is
continually growing, with 685 “open” problems out of 1105 total as of October 31, 2025;
the problems posed by Erdős over the years are scattered across many disorganized sources.
Unfortunately the solutions are also often similarly scattered, and so it can be very challenging
to determine for sure whether a particular problem is actually open, or if it was instead solved
decades ago and forgotten to most mathematicians. This has resulted in a non-trivial amount
of futile effort by mathematicians trying to solve problems that turned out to already be
known. For instance as a graduate student, the first author spent a bit of time thinking about
[Blo, Problem #490] and even mentioned it an informal meeting with other graduate students
as a beautiful problem. Eventually the first author was able to locate a reference by [Sze76]
solving the problem, with the vague title “On a problem of P. Erdős”. Thus, in practice a
problem being listed as “open” roughly indicates that at least 1 professional mathematician
attempted and failed to find a previously published solution by searching the internet. We
discuss our successful attempts to improve the status of this database using GPT-5, which led
to the following progress:

• Locating previously published solutions to 10 problems not previously marked as known:

223, 339, 494, 515, 621, 822, 883 (part 2/2), 903, 1043, 1079.

• Reporting noteworthy partial progress in the existing literature for 10 other problems:

32, 167, 188, 750, 788, 811, 827, 829, 1017, 1011.

• Correcting a misprint in the statement of problem 1041.

• Generating a new idea for Problem #848, which, together with previous suggestions
by online commenters van Doorn, Weisenberg, and Cambie, enabled us to settle the
problem.

In this section we focus on several examples from the first two bullets above. Problem

#848 and its solution are presented later in Section IV.1.

II.2.2 Literature search

The database of Erdős problems serves as a testbed for the abilities of GPT-5. For every
problem listed as “open”, we asked GPT-5 to locate a solution if it existed, and otherwise to

24

report significant partial progress that merits being mentioned on the website. This was done
using a combination of the ChatGPT interface, the OpenAI API, and internal OpenAI tools;
the latter two allow automated queries of many problems in parallel, and automated filtering
of responses also using GPT-5; aside from the automated aspect, we expect similar results for
any particular problem could have been obtained purely using the ChatGPT interface. The
prompts used were completely problem agnostic, consisting of a link to or a screenshot of the
problem, and an instruction to search the internet for a solution or partial progress, despite
the “open” designation on the website.1

The references provided were generally accurate and easy to verify manually. We did not
observe any cases in which GPT-5 pretended to have found a correct reference (though in
some cases it was overly enthusiastic about partial progress it had found). We also emphasize
that the problem numbers used correspond to their labeling on the database, beginning in
early 2024. In particular these numbers cannot be used in websearches to locate pre-2024
literature on a given problem.

Typical practice for a mathematician attempting to find literature is either following
long chains of references or attempting to “guess the correct search phrase” hoping to find
relevant literature. With GPT-5, one can skip this artificial middle step and simply plug
in the statement! Furthermore in mathematical practice, one is often tasked with locating
literature in a separate field “which surely exists”; GPT-5 can do so provided often vague
descriptions or candidate “theorem” statements.

We highlight a few representative cases. The first is problem [Blo, Problem #339], the

surprising success of which was the impetus for our efforts.

Problem #339 ([EG80]): Let A ⊆ N be an additive basis of order r, i.e. such that every
sufficiently large integer is the sum of at most r elements of A. Must the set of integers
representable as the sum of exactly r distinct elements from A have positive lower density?
(I.e., must this set occupy a non-vanishing fraction of the integers {1, 2, . . . , N } up to N ,
for all sufficiently large N ?)

A difficulty in searching for references is that Problem #339 is raised in a 100-page paper
of Erdős–Graham [EG80] which lists many other problem and has roughly 700 citations.
Sorting through to find which of these are relevant for the specific problem above would be
extremely time-consuming for a human. However, GPT-5 Pro was able to find the solution
in the first query we tried, given only a screenshot of the problem webpage. The discussion
in which GPT-5 Pro found the solution is here: [Saw25a]. We note that before the solution
was found, the problem attracted some discussion among participants on the online forum
https://www.erdosproblems.com/forum/thread/339.

Further Examples

Here we present three more examples, which further illustrate the challenges present in
mathematical literature search. We reiterate that the numbers below correspond only to the
recently created online database of Erdős problems, and thus are not useful for searching the
pre-2024 literature.

1We were not alone in such endeavors; see e.g. https://www.erdosproblems.com/forum/thread/737, https:

//www.erdosproblems.com/forum/thread/936, https://www.erdosproblems.com/forum/thread/1008.

25

Problem #515 ([Erd61]): Let f (z) be an entire function, not a polynomial. Does there exist
a path γ tending to infinity such that, for every λ > 0, the integral R
γ |f (z)|−λdz is finite?

Partial result toward Problem #750 ([Erd69]): Fix ε > 0. Must there exist a graph G of
infinite chromatic number so that every m-vertex subgraph has an independent set with at
least ( 1

2 − ε)m vertices?

Problem #1043 ([EHP58]): Let f ∈ C[x] be a monic polynomial. Must there exist a straight
line ℓ such that the projection of {z : |f (z)| ≤ 1} onto ℓ has length at most 2?

GPT-5 located references solving the above problems, respectively in [LRW84], [EHS82],
[Pom61]. Identifying these decades-old papers required GPT-5 to go far beyond the function-
ality of a search engine, and indeed to read each of these papers in detail and apply a genuine
understanding of mathematics.

In the first case, the work [LRW84] actually proves something more general than the
statement of Problem #515. Namely, the authors study integrals of e−F (z) where F : C → R
is a subharmonic function, meaning its average value on any circle is smaller than its value at
the center. It is a classical fact that F (z) = log |f (z)| is subharmonic when f is entire, which
means that the results of [LRW84] do imply Problem #515. However this connection is not the
focus of the paper, and indeed neither Erdős nor his above problem are mentioned. This use
of different vocabulary for similar underlying concepts is a common challenge in searching the
mathematical literature, and would have made [LRW84] quite hard to find using a standard
websearch. Indeed the main feature of Problem #515 is the integral expression, which would be
quite challenging to search for directly. GPT-5 in fact later located a corroborating reference,
a (256 page!) survey [HL18] that mentions the connection between [LRW84] and Problem
#515 on page 27.

Additionally, there is a subtle technical point in connection between [LRW84] and the
problem it solves, which is that the function log |f (z)| has a singularity wherever f (z) = 0.
This means that to apply the result from [LRW84] to Problem 515, it is crucial that their
definition of a “subharmonic” function allows such singularities. The paper requires a careful
reading to confirm that their definition of subharmonic is sufficiently general for the required
implication; here as well, GPT-5 assisted us in pointing out parts of the paper that made the
intended definition clear.

For Problem #750, the result from [EHS82] actually concerns the slightly different problem
of finding a bipartite subgraph, and shows such subgraphs exist with (1 − ε)m vertices. GPT-5
both found this reference and observed that taking the larger side of the bipartition solves the
Erdős problem in question. This (easy) deduction was done by GPT-5 during its literature
search without additional prompting, and to our knowledge came from GPT-5’s own reasoning.
Needless to say, an pre-LLM websearch would be unable to supply this missing argument to
connect the two results.

Problem #1043 is stated in [EHP58] together with 15 other problems; several of these
including 1043 were solved in [Pom61]. Unlike the others, Problem #1043 is addressed in a
brief side comment between two theorems, and was consequently overlooked by many including
the MathSciNet review of [Pom61]. Additionally it turns out that [Pom59] (displayed as
reference [10] in the screenshot of [Pom61]) actually contains the main work in solving Problem
#1043, and is written in German. GPT-5 translated and explained the proofs from [Pom59]
to us so that we could verify them ourselves.

26

Figure II.3: Pommerenke’s solution to Problem #1043, appearing on page 7 of [Pom61]. The
proof follows by combining the “approximation theorem” from [Pom61] and Pommerenke’s
previous work [Pom59] (cited as [10, p. 73] in the red block above). See https://www.
erdosproblems.com/forum/thread/1043 for a more detailed outline.

II.2.3 Looking forward

Motivated in part by our efforts, other researchers including Thomas Bloom and Terence
Tao soon started posting negative literature searches to the Erdős problem website, re-
porting that GPT-5 and other large language models were unable to locate an existing
solution to various problems. This provides a convenient, crowd-sourceable “soft certifi-
cate” that the solution is unlikely to appear in the published literature. See e.g. https:
//www.erdosproblems.com/forum/thread/689, https://www.erdosproblems.com/forum/
thread/915, https://www.erdosproblems.com/forum/thread/990.

The landscape of mathematical results is substantially broader than any one mathematician
plausibly has available at their fingertips. GPT-5 therefore provides the practicing mathemati-
cian a new mechanism to access the collective breadth of the mathematical literature.

27

II.3 Clique-avoiding codes: a cautionary tale

The following was communicated by Venkatesan Guruswami and Parikshit Gopalan, edited by
us and included with their permission

A binary linear code is a subspace of FN

2 ≃ {0, 1}N that avoids any nonzero vector of
low Hamming weight. Here we consider a variant where the code must avoid a collection of
structured vectors, namely all cliques in an n-vertex graph. This question is originally due
to [GY15], motivated by the study of maximally recoverable codes, although it was never
published.

(cid:1) coordinates of
Let us define the problem formally. Let [n] = {1, . . . , n} and index the (cid:0)n
2
F(n
by unordered pairs {i, j} with 1 ≤ i < j ≤ n. For a vertex subset S ⊆ [n], let χS ∈ F(n
2)
2)
2
denote the indicator of the edge set of the clique on S, i.e., χS({i, j}) = 1(cid:2)i ∈ S and j ∈ S(cid:3).
A binary linear code C ⊆ F(n
2)

is clique-avoiding if χS (cid:60) C for every S ⊆ [n] with |S| ≥ 2.

2

2
The question posed to GPT-5 was the following:

What is the minimum number of parity checks (i.e., the minimum
co-dimension) denoted r = r(n), of a clique-avoiding code as a
function of the number of vertices n?

A simple probabilistic (random coding) argument, which GPT-5 also found right away

(with some additive O(1) slack), gives the following upper bound:

Lemma II.3.1. We have r(n) ≤ n.

It is tempting to think that the random coding bound is tight, but there is no obvious
lower bound. This was the question we posed to GPT-5 (without really thinking about it
ourselves). Initially, it was convinced that this bound was tight (up to an additive constant),
and tried to convince us of this using a sequence of buggy arguments, resorting to linear
algebra and proof by authority:

• The most amusing was a hallucinated response to the effect that one of us had asked
this question on TCS Stack Exchange, and the answer given to them was n. Both these
claims are incorrect.

• Another incorrect argument nevertheless contained the germ of the main idea.

It
was to consider quadratic forms Q1, . . . , Qr and then look at the syndrome S(x) =
(Q1(x), . . . , Qr(x)) and claim that x 7→ S(x) is an injection, hence r ≥ n. This claim
is also false. This assertion vaguely resembles arguments used for standard syndromes
which are linear.

• When challenged about this, GPT-5 came up with an elegant (and correct) algebraic
argument based on the classical Chevalley-Warning theorem, which established Theo-
rem II.3.1 below. (On other attempts, it would come up with the correct formulation
using quadratic polynomials modulo 2, but then derive from it weaker results like
r ≥ ⌈(n − log(n))/2⌉.)

28

Theorem II.3.1. For all n ≥ 2, r(n) ≥ ⌊n/2⌋.

We later discovered a construction (without using GPT-5) showing that the lower bound
is exactly tight: the simple probabilistic bound above is suboptimal, and in fact r(n) = ⌊n/2⌋
for all n.

II.3.1 An unexpected twist

We were very excited to have an exact solution to a question which we had been curious
about for years, especially because one direction of the proof was due to an AI. However, we
soon learned that the same tight bound (as well as a different proof of the matching upper
bound) had appeared on Arxiv nearly 3 years previously in the short paper [Alo24]. In fact,
GPT-5’s proof of the lower bound was exactly the same as Alon’s! Thus, it appears that
GPT-5 reproduced Alon’s proof and passed it along to us without realizing its source. As it
turns out, a fresh attempt at querying GPT-5 Pro was able to recover this source, as depicted
in Figure II.4.

Our experience illustrates a pitfall in using AI: although GPT-5 possesses enormous internal
knowledge and the capability to locate even more using the internet, it may not always report
the original information sources accurately. This has the potential to deceive even seasoned
researchers into thinking their findings are novel. We expect that our experience is not unique,
and urge others to take special care in attribution when working with LLM-assisted proofs.

29

Do you know anything about the following question? This concerns

(n
2)
the minimum co-dimension of a binary linear code C ⊂ F
2 which
avoids the indicators of all cliques (when the codeword positions
are naturally viewed as edges of the complete graph on n vertices).

Answer. Yes. The minimum possible co-dimension of a binary linear code C ⊂ F(n
2)
that contains no clique indicators is exactly

2

⌊n/2⌋ .

Explanation (sketch). A linear code C of co-dimension r is the common kernel
of r parity checks. Equivalently, we select graphs G1, . . . , Gr on [n], and a vector
x ∈ F(n
2)

is in C iff

2

⟨x, 1E(Gi)⟩ = 0

for all i.

Avoiding all clique indicators means that for every nonempty vertex set T ⊆ [n], the
clique KT satisfies

|E(KT ) ∩ E(Gi)| ≡ 1

(mod 2)

for at least one i.

Upper bound: codim(C) ≤ ⌊n/2⌋. For odd n, partition [n − 1] into blocks
Bi = {2i − 1, 2i}, so there are (n − 1)/2 blocks. Define Gi to consist of all triangles
with base Bi and apex any vertex j > 2i. One checks that every clique KT has
odd intersection with at least one Gi, hence its indicator is excluded from C. Thus
co-dimension (n − 1)/2 suffices. A small modification handles even n, giving the
general bound

codim(C) ≤ ⌊n/2⌋.

Lower bound: codim(C) ≥ ⌊n/2⌋. Suppose r < n/2. A Chevalley–Warning type
argument produces a nonempty even-sized vertex set T ⊆ [n] such that |E(KT )∩E(Gi)|
is even for every i. Then the clique indicator of KT satisfies all parity checks and lies
in C, a contradiction. Hence r ≥ n/2 for even n, and therefore

Conclusion. Both bounds match, so the exact minimum co-dimension is

codim(C) ≥ ⌊n/2⌋.

For a full, elegant presentation, see N. Alon, Graph-Codes, Theorem 1.6.

⌊n/2⌋ .

Figure II.4: An interaction in which GPT-5 Pro was able to locate the reference [Alo24] solving
our problem. This query was made after we first became aware of [Alo24].

30

Chapter III

Working in tandem with AI

III.1 Recent experiences of using LLMs as research partners –

Timothy Gowers

This is a brief report on one or two recent experiences of using LLMs while actually doing
mathematics, by which I mean thinking about research-level problems at an earlyish stage
where I have some ideas but have not yet found an approach that feels likely to work. I can
summarize the situation so far by saying that I have not (yet) had the experience of an LLM
making a decisive contribution, but I have found them useful in the following ways.

1. If I suspect that a definition or mathematical observation I have come up with is in
the literature, an LLM can often help me find it. GPT-5 seems to be significantly
better at this than GPT-4. In particular, with GPT-5 my experience has been that
the references are rarely hallucinated, and even the hallucinations can turn out to be
pointers to references that exist and are useful.

2. I have sometimes formulated well-defined subproblems that look as though they shouldn’t
be too hard but also look as though they would take a bit of time to solve, and GPT-5
has solved them for me in a matter of seconds.

3. I have had reasonably precise ideas for solving problems that I have run past GPT-5

Pro, which has explained to me why they cannot work.

On the negative side, if I ask more open-ended questions, or offer more sketchy ideas for
proof attempts, then that seems to encourage the more annoying characteristics of LLMs to
come to the fore: they will tell me that my ideas do indeed work, and will write something
that supposedly fleshes out the details but that does not withstand close scrutiny. This applies
even to GPT-5 Pro, to which I have had access for the last few days (at the time of writing).
Taking this together, my current assessment of LLMs is that they are just beginning to be
useful as research collaborators: one can bounce ideas off them in the way that one can with a
human collaborator and one gets a response extremely quickly. As with human collaborators,
even the less good ideas of an LLM can sometimes stimulate me to make progress. (I think
of this as the “That clearly doesn’t work ... but wait a minute!" phenomenon.) So we have
reached the stage where LLMs can speed up the process of thinking about a problem, especially
if that problem is a little outside one’s primary domain of expertise, but we have not yet
reached the stage where an LLM is likely to have the main idea for solving a difficult problem.

31

(That last statement has to be interpreted carefully. I may find a problem difficult because I
am unfamiliar with the domain-specific tricks needed to solve it, but if the problem would be
found easy by somebody with the relevant expertise, then it may well be easy for an LLM
as well. Thus, by “difficult” I mean a problem for which the standard bag of tricks in the
relevant area does not suffice.)

I shall give two examples here of recent interactions with GPT-5 and GPT-5 Pro. I stress
that in each case, the main contribution of the LLM was to save me a small amount of time:
the observations it made and proofs it provided are ones that I would have expected to find
for myself before all that long, but with the help of an LLM I did not to have to make any
effort at all and I got the answers within seconds.

It is interesting to think about whether I should have made more effort before turning to an
LLM for help. Everybody will have to make their own decision about this. My current feeling
is that this issue is a bit like the issue of whether one should use calculators to do arithmetic.
One is clearly a much better user of calculators if one knows how to do the calculations oneself
and uses the calculators only as a time-saving device. Similarly, in these early days, I feel
much more comfortable asking an LLM to help me if I think it is carrying out a task for which
I have a good idea of how I would set about it and that I could almost certainly do without
help, given a bit of time. I would be less comfortable simply feeding it a difficult problem and
hoping for the answer (and also, for now, much less confident that I would get a useful answer
if I did so).

Example 1: a compact subset of L2

I was trying to solve a problem where I had a somewhat complicated non-linear map T : L2 →
L2 and wanted to prove that the iterates f, T f, T 2f, . . . of a certain function f converge. From
what I knew about T , it was fairly clearly enough to show that the iterates had a convergent
subsequence, so I was looking for a compact subset K ⊂ L2 that I could argue contained all
the iterates.

I knew that in an arbitrary Banach space, one source of compact sets is convex hulls of
null sequences, and after a little thought I realized that I could prove that in ℓ2, if I took
any non-negative square summable sequence s, then the set {a ∈ ℓ2 : |a| ≤ s} was compact.
I had reason to believe that the iterates of f would exhibit something like Gaussian decay,
so I wondered whether a similar result was true for the set of functions bounded above by a
Gaussian.

I soon realized that that could not be the case, because one can take a sequence of more

and more rapidly oscillating functions, and those will have no convergent subsequence.

However, I had reason to believe that the iterates of f under the map T would not start
oscillating more and more, so I did not lose hope. It occurred to me that the oscillations should
make their presence felt on the Fourier transform side, and as the operation T interacted well
with the Fourier transform, it might be enough for me if the set of functions f such that both
f and ˆf are bounded above by a Gaussian is compact.

This felt plausible to me, so I asked GPT-5 about it. After 22 seconds, it provided me
with a proof, which relied on a lemma that I had not heard of called the Kolmogorov-Riesz
lemma. The proof looked correct, given the lemma, and the lemma looked plausible. I checked
online and found that it did indeed exist. I don’t know how long it would have taken me to
realize that the lemma was the right intermediate step (after which I could have used GPT-5

32

for its semantic search capabilities) but I estimate that this saved me anything between an
hour and three hours of work.

There is an epilogue to this story. I am collaborating on the project with two research
students, who were also good at pointing out drawbacks with my proposals for proof approaches.
In particular, one of them pointed out that the condition that f and ˆf should both be bounded
above by a standard Gaussian would be too strong, since Hardy’s uncertainty principle implies
that in this case f must itself be a multiple of a standard Gaussian. Thanks to this comment,
I realized that there was a mismatch between the condition I thought I could obtain – that
both f and ˆf are bounded above by a Gaussian e−cx2, but not necessarily with c = 1/2 –
and the condition that I had asked GPT-5 about. So I then asked GPT-5 whether one still
had compactness under this weaker assumption. Again, very quickly it told me the answer,
which in this case was one that I would have realized for myself fairly quickly, but not as
quickly as GPT-5. Unfortunately, the answer was negative: the Hermite functions (which are
eigenfunctions of the Fourier transform) form a sequence satisfying the condition and having
no convergent subsequence.

I was fooled by this, because each Hermite function is a polynomial multiplied by e−x2/2
and therefore decays more quickly than e−cx2 if c < 1/2. However, while writing the previous
paragraph I started to have my doubts, since what I was after was a sequence of functions all
dominated by Ce−cx2 for some fixed C and c, and although each Hermite function is eventually
dominated by e−cx2 when c < 1/2, that is a weaker statement. Since I could now ask GPT-5
Pro I asked whether there could be a single function of the form Ce−cx2 with c > 0 that
dominates all Hermite functions. It told me that the answer was no and gave me the following
convincing proof. The Hermite functions satsify the recurrence relation

xhn =

r n + 1
2

hn+1 +

r n
2

hn−1.

Since the Hermite functions are orthonormal it follows that

Z ∞

−∞

x2hn(x)2 dx =

n + 1
2

+

n
2

= n +

1
2

.

It follows that for sufficiently large n it is at least C2 R x2e−cx2 dx (since the latter is finite
and independent of n), which proves that for sufficiently large n |hn| cannot be dominated by
Ce−cx2.

I then asked GPT-5 Pro whether if 0 < c < 1/2 the set of functions f such that |f | and
| ˆf | are dominated by Ce−cx2 is compact. It told me the answer was yes and gave a convincing
looking proof, but one that I felt I needed to check, especially as it said that a certain step
of the deduction followed from a “semigroup version" of Hardy’s uncertainty principle but
gave no details about what that version actually said. When I asked for more detail it gave
me a hallucinated reference but by an author who had written on closely related topics. I
complained that I couldn’t find the paper, and this time it gave me a reference to a paper
that was highly relevant, though it got the names of the authors wrong. Unless I am making
another mistake, this paper contains exactly the result I wanted: under the giving conditions
it gives an upper bound for the Hermite coefficients of f and this upper bound decays at an
exponential rate. Thus, there is a null sequence with a convex hull that contains all such
functions f , giving us the desired compactness.

The final twist in the story is another remark by my human collaborators. I had been
slightly careless in assuming that the set of all f such that |f | and | ˆf | are bounded above by

33

Ce−cx2 is invariant under T , so the compactness result we now have is not obviously sufficient
for our purposes. Nevertheless, we have taken a significant step forward in our thinking about
the problem, and a back-and-forth with GPT-5 and GPT-5 Pro greatly sped up the process.

Example 2: graphs with uncharacteristically small clique number

A very nice problem I recently heard about asks whether there is an algorithm with the
following properties.

1. It takes as its input a graph with n vertices.

2. If the graph contains no clique of size (3/2) log2 n, then it outputs 1.

3. For almost all graphs (meaning a 1 − o(1) proportion, but even 99% would be interesting),

it outputs 0.

4. It runs in polynomial time.

The reason one might hope for such an algorithm to exist is that random graphs have clique
number approximately 2 log2 n with high probability, and will typically have many cliques of
size (3/2) log2 n, which will be highly dispersed, so getting rid of all of them seems difficult.
Therefore, one expects that a graph with no clique of size (3/2) log2 n will have to have
“unexpected structure" of some kind that can be picked up by an efficient algorithm. (By
contrast, adding an unexpectedly large clique is easy to achieve: one just adds it in at random.
n/ log n.)
Such “planted cliques" appear to be very hard to find if they are of size smaller than
I decided to try to think of ways of creating a class of graphs that did not have cliques of
size (3/2) log2 n but that nevertheless looked random and therefore hard to detect efficiently.
Each idea I had, GPT-5 Pro was able to knock down in seconds (always softening the blow
first by telling me that it liked the idea).

√

First idea: use linear algebra

A well known result in combinatorics, and a surprising one when you first see it, is that the
number of subsets of {1, 2, . . . , n} you can have of odd size if all their intersections are of even
size is n. The proof is one line: interpret each subset as a 01-sequence in the usual way, and
regard the sequence as a vector in Fn
2 . The parity of the intersection of two sets becomes the
F2-inner product of the two corresponding vectors, and the usual proof that an orthogonal
sequence of vectors is linearly independent can be easily adapted to show that sets of odd size
and even intersections correspond to linearly independent vectors, and hence that there are at
most n of them.

If we form a graph where the vertices are all sets of odd size and two vertices are joined by
an edge if their intersection has even size, then the above result tells us that there can be no
clique of size greater than n. Since there are N = 2n−1 vertices, this graph has no clique of
size (3/2) log2 N (for sufficiently large N ).

So far, it is easily distinguishable from a random graph: for example, it is much more
regular than a random graph will typically be. I proposed to deal with that problem by
passing to a random subset of the vertices of size 2αn for some α reasonably close to 1.

34

GPT-5 Pro pointed out to me that the rank over F2 of such a matrix would be at most n,
whereas the rank of the adjacency matrix of a random graph on 2αn vertices would typically be
close to 2αn, so there was a very easy algorithm for picking out my proposed class of matrices.

Second idea: use angles between unit vectors

My first idea had failed because it was in some sense too algebraic, but it occurred to me that
I knew another result about vectors that implies the existence of a graph without large cliques.
It states that the largest number of vectors you can find in Rd such that any two of them have
a negative inner product is d + 1 (with equality for the vertices of a regular simplex centred at
the origin). So I wondered about choosing d in such a way that d + 1 < (3/2) log2 n and then
choosing n random unit vectors u1, . . . , un in Rd, joining two of them if and only if they have
negative inner product. This felt to me as it had a chance of working, because although the
matrix with entries ⟨ui, uj⟩ has rank d (with probability 1), we are applying to each matrix
entry the rather violent and unalgebraic operation of replacing it by 1 if it is negative and 0
otherwise.

However, GPT-5 Pro almost immediately gave me a test for detecting matrices created
this way, based on the fact that the larger ⟨ui, uj⟩ is, the more likely it is for a random vector
to have a positive inner product with both of them. Indeed, if ui and uj form an angle θ, this
probability is 1/2 − θ/2π. From simple calculations it follows that in any graph obtained by
my proposed method, there will be noticeably more independent sets of size 3 than one would
expect in a random graph, and there will also be many pairs of vertices with neighbourhoods
that intersect a lot more than expected.

I then noticed that a test of this form also applies to the Fn

2 construction, since if
x + y + z + w = 0 then any vertex v that is joined to x, y and z is automatically joined to w.
Passing to a subset of size 2αn with α fairly close to 1 leaves many quadruples x, y, z, w of
vertices that add to 0, which will have neighbourhood intersections that are much too large.

Third idea: replace linear algebra by polynomials

I wondered rather vaguely whether one could rescue the more algebraic approach by devising
some polynomial condition for two elements x, y of Fn
2 to be joined. That is, perhaps I could
find some suitable polynomial map P : Fn
2 → F2 (symmetric in x and y) and put an edge
between x and y if and only if P (x, y) = 1.

2 × Fn

This time GPT-5 Pro gave me a longish answer explaining that unless the degree of P was
huge the resulting matrix would still have low rank. That seemed believable but I did not
check the argument carefully. Asking GPT-5 Pro the same question again (just about the
rank of such a matrix) I obtain the following simple argument: this one would probably have
taken me a few minutes to find for myself, so once again there is speed-up but not as much
as with some of the other questions. Let P have degree at most d. If we consider the xth
row of the matrix, then its values are P (x, y) as y ranges over Fn
2 . For fixed x, P (x, y) is a
polynomial in y of degree at most d, so it is spanned by the monomials xi1 . . . xis with s ≤ d,
and therefore the row is spanned by these monomials when we consider them as functions
(cid:1). For this not to
from Fn
be noticeably smaller than 2αn (I am imagining that I have also passed to a random subset of
the vertices of size 2αn) we would need d to be linear in n.

2 to F2. In particular, the row space has dimension at most Pd

(cid:0)n
s

s=0

But GPT-5 Pro also told me that if the degree of P was as large as that, there would be

35

no obvious reason for the graph not to contain a clique of size (3/2) log2 N , where N is the
number of vertices. If I remember correctly, it didn’t give a particularly compelling argument
for that, but it agrees with my intuition to the point that I do not see a need to defend the
assertion in detail.

Attempting to go in the other direction

Given that I had a uniform explanation for the failure of my first two attempted examples, I
then decided to have a conversation about whether a graph that passes certain strong tests
for randomness based on the sizes of the intersections of neighbourhoods could be shown to
contain a clique of size (3/2) log2(n). The answers I received were unhelpful, which was not
very surprising as my question was too open ended and very little appears to be known about
quasirandomness conditions that give information about the counts of subgraphs of size as
high as (3/2) log2 n.

Conclusion

As a research supervisor I have a rule of thumb for when a contribution I make to the research
of one of my PhD students is at the level where I should be a joint author. The rule is that if
the student comes to discuss the problem with me and I have, in the course of that discussion,
an idea that comes more naturally to me than to them and that turns out to be helpful, then
that is not enough for joint authorship. But if I spend time struggling with the problem (of
course, I will do this only if the project is officially a joint one) and during the course of
that struggle I come up with an idea that required more than just standard expertise that I
happened to have, then I have made a genuine contribution to the work.

My experience so far with LLMs is that they are capable of playing this knowledgeable-
research-supervisor role with me, which can be extremely useful given just how much knowledge
they have, but that they are not yet at the level (or at least have not yet exhibited that level in
my own interactions with them) at which a human mathematician who follows my convention
above would ask for joint authorship.

36

III.2 Power spectra of gravitational radiation from cosmic

strings – Robert Scherrer

III.2.1 The problem in context

Cosmic strings are hypothetical one-dimensional topological defects that might have formed
at a cosmological phase transition in the early universe (see, e.g., [HK95] for a review). After
a burst of interest in the 1980s, interest in cosmic strings declined sharply. However, there has
been a recent revival of interest in these objects because they can serve as copious emitters
of gravitational radiation such as the stochastic background recently observed by the Pulsar
Timing Array.

In a previous paper [SS24], a graduate student (David Storm) and I examined the spectrum
of radiation that might arise from the class of cosmic strings first examined by Garfinkle and
Vachaspati [GV87]. While the total power emitted per unit solid angle of the gravitational
radiation for these cosmic strings was investigated in [GV87], the quantity relevant for the
stochastic background is the total power Pn as a function of the nth harmonic. The power
emitted at frequency ωn = 4πn/L (where L is the length of the loop) by a cosmic string with
a mass per unit length µ in the direction of the unit vector ˆr is given by [GV87]

Pn =

Z

32Gµ2
π3n2

dΩ

[1 − (−1)n cos(nπe1)][1 − (−1)n cos(nπe2)]

(1 − e2

1)(1 − e2
2)

,

(III.1)

where e1 = ˆr · ˆa and e2 = ˆr · ˆb, and ˆa and ˆb are unit vectors whose directions characterize the
string trajectory. Since ˆa and ˆb are three-dimensional unit vectors, the problem is characterized
entirely by the angle α between ˆa and ˆb. Square loops have α = π/2, while smaller values of α
correspond to more prolate rectangular loops.

In our earlier paper [SS24], we examined a range of values for α and found numerically that
dPn/dΩ did not scale as n−2 at large n, as had been previously argued [DV01], but seemed
to diverge from the predicted result. As our results disagreed with received wisdom, it was
important to confirm them analytically.

Human route. After roughly six months attacking the problem, I was able to derive a
partial result, but only for the simplest case: α = π/2. Furthermore, my result applied only
to even n. In this case, I found

Pn =

#

"

128Gµ2
π2n2

[γ + ln π + ln(n)].

(III.2)

where γ is Euler’s constant. The details of the derivation can be found in the appendix of
[SS24].

III.2.2 The task given to GPT-5 Pro: goal, prompts, and model behavior

Goal. While my eventual goal was to determine analytically the power spectrum for all
α, as a first step I asked GPT-5 Pro to derive Pn—or rather, the integral appearing in
Eq. (III.1)—for odd values of n. I knew from our numerical results that in the large-n limit,
it almost certainly had the same form as Eq. (III.2), but the methods I used to derive this
result could not be applied to the odd-n case.

37

In my initial prompt, I asked GPT-5 Pro to evaluate the integral in Eq. (III.1)
Prompt.
for odd n, and give the asymptotic behavior for large n. The program hung up for quite
a long time, giving me no details about its thought process. After several hours I became
frustrated and killed it. I then repeated the prompt but with an added reference to the Storm
and Scherrer paper. The exact prompt was:

I would like to analytically integrate the following integral over
the sphere, so that theta goes from 0 to π in spherical coordi-
nates, and phi goes from 0 to 2π. The integral is integral of
[1 + cos(nπe1)][1 + cos(nπe2)]/[(1 − e2
2)] where n is an odd
integer, and e1 = cos(θ) and e2 = sin(θ) sin(ϕ), where θ and ϕ are
the usual coordinates in spherical coordinates: θ is the angle relative
to the z axis, and ϕ is the azimuthal angle. I would like an exact
solution for all odd n, but you may assume that n is much larger
than 1 if you cannot solve the general case. Note that a solution for
even n was derived by Scherrer and Storm in their paper in Physical
Review D.

1)(1 − e2

I was particularly pedantic about the definitions of θ and ϕ because in spherical coordinates,
physicists use the opposite convention from mathematicians, and I was not sure which
convention GPT-5 Pro would default to. I also took n to be odd in Eq. (III.1) and dropped
the (−1)n factors.

Outcome. After reasoning for 40 minutes, GPT-5 Pro produced a result for large odd n
identical to the asymptotic result I had previously derived for large even n [Eq. (III.2)]. It
used a completely different method of solution from my own. While my own solution involved
splitting the integral into three pieces and taking an ad hoc approach to each piece, GPT-5
used an expansion in Legendre polynomials, and then extended this to a Bessel function
expansion. I had suspected that some sort of solution in Bessel functions was possible but had
never found it myself. GPT-5 Pro also gave the leading order correction term to this formula,
so that it becomes

4π[ln(nπ) + γ − Ci(nπ)],

(III.3)

where Ci is the cosine integral function. I was not aware of this correction term, which is
relevant for small values of n. Further, GPT-5 Pro provided a table illustrating the numerical
integration in comparison with the large-n asymptotic result, showing that they diverge from
each other at small n (less than 5) but are almost identical at large n. (I should mention that
even the numerical integral is difficult—the integrand in angular coordinates is so spiky that it
resembles a sea urchin). GPT-5 Pro also asserted (correctly) that the integral in the prompt
does not converge for even n, but this is because I left out the (−1)n factors from Eq. (III.1).

For reference, the full interaction is available here: [Sch25].

III.2.3 Result, implications, and next steps

Results. GPT-5 Pro derived a new analytical result that confirmed our earlier numerical
simulations for the gravitational wave power spectrum from a class of cosmic strings. Had
this result been available to me earlier, I would have incorporated it into Ref. [SS24].

38

Implications. GPT-5 Pro is capable of solving complex analytic integrations that are
beyond the reach of symbolic manipulation programs such as Mathematica.

Next Steps. The obvious next step will be to solve the general case for arbitrary α. If
successful, this will provide a sufficiently important result to motivate a research publication.
I have accumulated a number of such unsolved interesting mathematical problems that have
frustrated me over my 40-year research career. Many of these seem particularly well-suited to
AI solution. I have long waited for this moment to arrive.

39

III.3 AI-assisted reduced-physics modeling of thermonuclear

burn propagation – Brian Keith Spears

Premise

GPT-5 has evolved to be a quite capable research assistant.
It is right now remarkable
for its moderate depth of knowledge (relative to, say, a 2-decade veteran PhD researcher)
striped across an incredibly broad range of topics. We test and exploit this capability here by
recruiting its assistance to formulate an inertial confinement fusion problem that probes this
breadth.

We aim to develop an improved understanding of the local physics of fusion ignition and
burn propagation at the interface between hot and cold nuclear fuel – a critical complex of
processes that lead to the production of more energy by fusion than the energy absorbed by
the ICF target [Abu+24]. To this end, we set up a reduced-physics model that isolates the
generation of a thermonuclear burn wave from the complex implosion dynamics of a full ICF
implosion simulation [Mar+96].

We are interested in AI-driven assistance with several problem phases:

• The setup of a simplified physics model with which we can develop intuition;

• The numerical simulation and solution of such a model;

• The design of numerical experiments to help us elicit the key physics sensitivities;

• Development of a theory that explains the numerical findings.

We have developed a simplified, static (no fluid motion) reaction–diffusion model for an ICF
deuterium–tritium (DT) capsule to investigate the ignition and propagation of a thermonuclear
burn wave into cold, dense fuel. This framework balances fidelity with transparency: it isolates
the relative roles of thermal conduction, alpha-particle energy deposition, radiation losses, and
density-profile shaping on wave propagation.

III.3.1

Introduction

To get going, I prompted GPT-5 to help me set up a model environment that I could use to
explore thermonuclear burn propagation in inertial confinement fusion (ICF). This physics
couples complex microphysics with steep thermodynamic gradients, and I would like to
intentionally remove the dynamics that continually and crucially changes the power balance.
The goal is simply to find the ideal fusion interface in the static case; later we can figure out
how to map it into a real implosion and control it (via shock timing, drive preheating, etc.).
I start in my usual way, with what is probably (perhaps not affectionately) known as a

“boomer prompt”:

40

I want to explore the dynamics of a thermonuclear burn wave
propagating into cold deuterium-tritium fuel in an imploding ICF
capsule. I want to build a simplified, static (no material motion)
model of the ignition and propagation of such a wave in spherical
geometry.
Let’s build a reaction diffusion equation to model it. I want the
model to have 3 regions. The first region is a hot, low-density DT
plasma (hot spot) that will initiate the burn wave. It should have
properties similar to the central hot spot of a NIF implosion at peak
ion temperature and should have similar radius. The next region
will be relatively cool, dense DT fuel with a quadratic density profile
(one parameter for average slope and one for curvature), ramping
from the density of the hot spot to high-density cold fuel and a
similar temperature ramp from hot to cold. The third and final
region is uniform density (relatively) cold fuel, also at the conditions
and thickness of a well shock-timed NIF implosion at peak shell
compression (after rebound shock passage).
The reaction diffusion equation dynamics should capture clearly
three effects: first, thermal energy should diffuse according to a
good approximation of the density- and temperature-dependent
thermal diffusivity; second, the reaction term should capture energy
production by alpha particle generation and the non-local deposi-
tion of that alpha energy consistent with the condition-dependent
stopping power of alpha particles in the nearby material; finally,
there should be radiation loss which is transported far away from
the local heat front.
With this model, we want to ask a key question. What density
profile in region 2 most easily allows us to propagate a burn wave?
We presume that for very shallow ramps, the hot spot can initially
heat region two easily, but wave propagation “flames out.” For very
steep ramps, we presume it requires more energy in the hot spot
to get the wave going, but that it propagates more easily. The
intuition, then, is that there is an optimal profile (average slope and
curvature).
Set up this model with appropriate physical constants and assump-
tions. Embed it in an optimization framework that delivers the
profile slope that provides the most robust initiation and propa-
gation of a burn wave. Demonstrate the results with a few plots
showing temperature wave propagation under a few scenarios, and
add a figure-of-merit plot with axes difference in propagation depth
(or speed) vs. fuel profile (slope and curvature).

This initial prompt led to an extended conversation with GPT-5 Pro, with a lot of back
and forth which I omit here. The remainder of this section summarizes the work that I was
able to carry out in conjunction with GPT-5 Pro. Most of the results given here were produced
by GPT-5; I merely provided it with some feedback and guidance. Interested readers may
consult this long exchange with GPT-5 Pro here: [Spe25].

41

III.3.2 Model Formulation

The result was a pretty good start at a static model – the formulation is good, the terms are
aptly formed and named, and the physics property choices are relevant. After some dialogue,
we settled on familiar and more detailed physics data, all of which were offered by GPT-5 in
advance of my requesting them. This is how the conversation would go with a national lab
colleague and expert.

Together, we consider spherical symmetry with no material motion. The dependent variable
is the local DT temperature T (r, t) in units of keV. Energy conservation for the DT plasma
(neglecting P dV work) is expressed as

ρCv

∂T
∂t

=

(cid:18)

1
r2

∂
∂r

r2ρ χ(T, ρ)

(cid:19)

∂T
∂r

+ Qα(T, ρ) − Qrad(T, ρ),

(III.4)

where Cv = 3
alpha-particle heating source, and Qrad the volumetric radiation loss.

2 kB/mi is the specific heat per unit mass, χ is the thermal diffusivity, Qα the

For thermal transport, we adopt a Spitzer–Härm scaling for electronic heat conduction

[Lym62; HL58]:

χ(T, ρ) = χ0

T 5/2
ρ

,

(III.5)

with χ0 = 0.12 chosen to approximate magnetized DT plasma diffusivity near 5–10 keV and
ρ ≈ 1 g cm−3. (The choice of magnetized values is debatable, and not conservative, but we
will let it slide for our experiment.)

The fusion power density follows from the Bosch–Hale [BH92] fit for DT reactivity ⟨σv⟩:

where nD = nT = ρ/(2mi). The alpha-particle heating source is

Pfus = nDnT ⟨σv⟩Efus,

Efus = 17.6 MeV,

Qα = fαPfus,

(III.6)

(III.7)

with fα = Eα/Efus = 3.5/17.6 ≈ 0.199.

We model finite-range alpha transport using a nonlocal Helmholtz operator [AM04]:

(cid:16)

I − λ2

α∇2(cid:17)

qα = fdepPα,

(III.8)

where λα(T, ρ) = Rm(T )/ρ and Rm(T ) is the alpha mass range (in g cm−2). The physically
realistic table adopted was

Rm(T ) = 0.30

(cid:18) T

(cid:19)1/2

3 keV

g cm−2,

(III.9)

consistent with 3.5 MeV alpha stopping data. Deposition was limited to 90% of total energy
(fdep ≤ 0.9) to mimic escape.

Finally, for radiation losses, we use an approximate gray diffusion with an escape probability

[MW84; Bra65]:

Qrad = Crad ρ2

√

T

1
1 + κ0ρRout

,

(III.10)

where Crad = 3 × 10−36 W m3 kg−2 keV−1/2 and κ0 = 5 × 10−3 approximate Rosseland mean
opacities for DT.

GPT-5 is quite good at this kind of model development and setup. It required little
intervention on my part to make this plausible and complete. That would change in the
numerics.

42

III.3.3 Numerical Implementation

GPT-5, without explicit prompting, selected for a partial differential equation (PDE) solving
a second-order Crank–Nicolson implicit scheme on a uniform radial grid (N ≈ 150 points).
Time integration proceeded with adaptive substeps up to 10 ns. Boundary conditions:

∂T
∂r

= 0 at r = 0,

∂T
∂r

= 0

at r = Router.

(III.11)

Reaction, diffusion, and radiative source terms were split via operator splitting. Alpha
deposition was solved by a tridiagonal Helmholtz inversion at each step.

These are reasonable choices for the kind of testbed we seek to develop. It should be noted
that this took a few minutes to code up. While I could have done this, I do not do it every
day, and I suspect it would have taken me a few work days to really get solid code working.
The point is that I can now deliver in minutes as if I were at the highest level I have ever been
at for this kind of work. There are those who will say this implementation is not remarkable,
and it is not. But, to have it in minutes is remarkable.

It was at this point that my human labor and intelligence were required. I needed to
suggest some parameterization for the density and temperature profiles at the fusion boundary.
GPT-5 also needed me to help it find the region of parameter space that exhibited the kinds
of threshold behaviors we were looking for. Make the hot spot too strong, and the whole thing
singularly explodes. Make it too weak, and no burn wave will initiate and propagate. To
understand the effect of the profile we are imagining, we need a delicately balanced marginal
system. Given the nonlinearities in ICF, this takes not only intuition, but some numerical
empiricism. GPT-5 made me work here.

For the density profile, three regions were specified:

• Region 1 (hot spot): r ≤ Rhs, ρ = ρhs ≈ 1.2 g cm−3.

• Region 2 (ramp): Rhs < r ≤ Rhs + Lramp,

ρ(r) = ρhs + s(r − Rhs) + c(r − Rhs)2,

(III.12)

scaled to ensure ρ(Rhs + Lramp) = ρcold.

• Region 3 (cold fuel): ρ = ρcold ≈ 150 g cm−3.

The ramp is controlled by the slope s and curvature c, spanning

s ∈ [1.5, 6.5] × 109 kg m−4

and c ∈ [−6.5, 0] × 1013 kg m−5.

I spent a few hours of my time adjusting hot spot conditions, cold fuel conditions, con-
ductivities, alpha particle stopping powers, and profile designs to get a physically reasonable
result. This activity is what we in ICF might call “design work” – getting to the place where
the physics are interesting and useful. GPT-5 was a bad designer, offering results that were
null, noisy, or invalid (NaNs), while claiming that glory had been achieved. However, when
re-prompted to examine a pathological result or null signal, GPT-5 offered quite sophisticated
solutions, including different implementations of FFTs to prevent aliasing, improved resolutions
to track burn fronts, altered performance metrics to amplify signals. If a user gave up at the
first wacky response, they would miss out on some rather powerful capability of the model.
It needs to be highlighted that this requires you to be a very confident physicist. You have

43

to know that an assertion is wrong, you have to confidently push for a better solution, and
you have to be good enough to know when a real solution has been reached. I often find that
GPT-5 offers solutions that I have to think deeply about to determine if they are actually
correct, or just convenient.

We settled on simulations that were performed at a fixed hot-spot temperature Ths =
1.5 keV. For each (slope, curvature) pair, the mean front speed of the 5 keV isotherm between
1–3 ns was measured by least-squares fitting:

Z t1

drf
dt
t0
t1 − t0

dt

.

vmean =

(III.13)

The resulting vmean(s, c) field was smoothed with a Gaussian kernel (10% bandwidth) to
remove interpolation noise. Contours of 40–100 km s−1 clearly revealed a propagation ridge
centered around

s ≈ 3.0 × 109 kg m−4,

c ≈ −4.5 × 1013 kg m−5,

(III.14)

with local maxima up to vmean ≈ 100 km s−1.

Figure III.1: The color map represents the mean propagation speed of a thermonuclear burn
wave propagating into the cold ICF fuel. That wave speed depends on the profile of the
interface between hot and cold DT fuel. More specifically, both the average slope and the
curvature play a key role. Together they set an optimum profile for delivering the highest
speed wave under the given physics assumptions. These optimal profiles lie along the yellow
ridge.

44

We spent some time examining the density profiles along the ridge. Five equally spaced

points along this ridge were selected; their parameters are listed in Table III.1.

# slope (kg/m4)
1
2
3
4
5

4.02 × 109
3.23 × 109
2.18 × 109
1.50 × 109
1.50 × 109

curvature (kg/m5)
−6.5 × 1013
−4.91 × 1013
−3.27 × 1013
−1.64 × 1013
0

Table III.1: Five equally spaced points along the propagation ridge in (s, c) space.

Figure III.1 illustrates that as curvature becomes less negative, the density near the
hot-spot boundary rises faster – stiffening the thermal coupling and slightly reducing the front
speed. Optimal profiles exhibit a gentle initial density rise, allowing alphas to preheat the
ramp before losses dominate deeper in the cold fuel.

Within a few hours of noodling, we had a decent numerical picture of a relationship between
the parameters chosen to describe the interface and their optimal values for propagating rapid
burn fronts.

III.3.4 Theoretical Explanation

While the numerics show a trend, a good design physicist will aim to explain that trend using
the theoretical tools at his disposal. I asked GPT-5 to help do this here for two reasons. First,
it is useful to have this theoretical picture for forming the intuition we are looking for. Second,
it provides an alternative check that GPT-5 has not hallucinated the physics and that I have
not been fooled. This is a much-needed verification process.

GPT-5 quite quickly settled (following my nudging prompt) on a traveling-wave power

balance to understand what sets the ridge.

We settled on a quasi–steady burn front, written in a co-moving coordinate ξ = r − vt.

Writing ρ = ρ(ξ) from the imposed ramp, the energy equation reads

−v ρCv

dT
dξ

=

(cid:18)

d
dξ

ρ χ(T, ρ)

(cid:19)

dT
dξ

+ Qα

(cid:2)T, ρ; ρ(·)(cid:3) − Qrad

(cid:0)T, ρ, ρRout

(cid:1).

(III.15)

It identified, I believe correctly, key scalings:

• Thermal transport: χ ∼ χ0T 5/2/ρ. Low ρ implies strong preheat (large χ), but also

dilute heat capacity (smaller ρCv).

• Fusion source: Qα ∼ fαEfus(nDnT )⟨σv⟩(T ) D[ρ]. With n ∝ ρ, this gives Qα ∝ ρ2⟨σv⟩(T )

times a nonlocal deposition factor D[ρ].

• Radiation losses: Qrad ∼ Cradρ2

√

T /(1 + κ0ρRout). Higher ρ amplifies emissivity but

also increases optical depth and reduces escape.

The ridge (maximal mean front speed) consists of (s, c) pairs for which the net power
into the preheat zone – from conduction plus alpha deposition – most efficiently overcomes

45

radiative loss and the enthalpy needed to lift cold fuel to the 5 keV ignition threshold. In
other words, where

(conduction from hot side) +
(nonlocal alpha deposition)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
{z
|
effective drive

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
}

is maximized against

(radiative loss) +
(downstream conduction) +
ρCvv∆T
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
|

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
}

{z
sinks + enthalpy

It argued for an optimal slope s. Write the ramp (endpoint-normalized) as

ρ(ξ) ≈ ρhs + s ∆ + c ∆2, ∆ ≡ (ξ − ξhs) ∈ [0, Lramp].

Make s smaller (shallower): the near-front ρ stays lower for longer distances.

(III.16)

(III.17)

• Pros:

larger χ ∝ 1/ρ implies better conductive reach; longer alpha stopping length

λα ∼ Rm/ρ implies alphas preheat ahead of the front.

• Cons: Qα ∝ ρ2 is weaker at the front; also the hot preheat layer is optically thinner, so

radiative escape improves.

Make s larger (steeper): the front sees higher ρ sooner.

• Pros: stronger Qα ∝ ρ2, larger heat capacity ρCv to “catch and hold” heat, greater ρR

so alpha energy deposits more locally (bigger fdep).

• Cons: χ falls as 1/ρ, so preheat is poorer; emissivity (∝ ρ2) rises, giving a larger radiative

sink (even if escape probability shrinks).

Net: there is a sweet spot in s where conductive/alpha reach is still good, but ρ at the

front is just high enough to bootstrap self-heating without being overly lossy.

It then moved on to optimal curvature c and why “moderately negative” values win.
Curvature redistributes where along the ramp the density rises:

• Negative curvature (c < 0) produces a gentle initial rise near the hot-spot edge and
a steeper rise later. This creates a “preheat shelf”: a low-ρ, optically thinner, high-χ
region right where alpha deposition from the hot spot wants to land (a few λα out). It
boosts nonlocal alpha coupling and conductive lift to 5 keV in the ramp before losses
take over.

• Positive curvature (c > 0) produces a steep early rise that quickly increases ρ just outside
the hot spot. That chokes preheat (low χ), increases emissivity, and shortens λα so more
alpha energy deposits behind the front rather than ahead of it – good for peak T behind,
bad for front advance.

This is why the data favor moderately negative curvature at nearly any slope that lies on
the ridge: it positions the right ρ at the right distance so the alpha–conduction synergy is
maximized where the front needs it.

Ultimately, a simple power-balance criterion predicts an “ideal” c for each s. A helpful
mental model is to focus on a control volume of thickness δ centered a distance ∆⋆ ahead of
the hot-spot edge, where the front is forming. Choose ∆⋆ ∼ a few local alpha ranges and

46

conductive penetration depths during 1–3 ns. Then require net positive power there to reach
and hold Tig ≈ 5 keV:

α (∆⋆)
Qin
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
}
{z
|
nonlocal from hot side

cond(∆⋆)
+ Qin
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
}
{z
|
∝χ ∆T /ℓ

≳

Qrad(ρ(∆⋆), T )
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
}
{z
|
√
∝ρ(∆⋆)2
T /(1+κ0ρRout)

+ ρ(∆⋆)Cv
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
{z
|
enthalpy over timescale τ

Tig − T0
τ
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
}

.

(III.18)

Two closure relations connect ρ(∆⋆) to (s, c):

• Alpha reach: λα(∆⋆) ∼ Rm/ρ(∆⋆) should be commensurate with ∆⋆, otherwise alphas

either overshoot (too low ρ) or deposit behind (too high ρ):

ρ(∆⋆) ∆⋆ ∼ Rm (order unity in areal density).

(III.19)

For a fixed s, this picks out a ρ(∆⋆) and thus fixes the needed curvature c via ρ(∆⋆) =
ρhs + s ∆⋆ + c ∆⋆2.

• Conduction match: the conductive preheat length in time τ is ℓc ∼

Efficient preheat wants ℓc ∼ ∆⋆, i.e.

√

χ τ ∼

q

(χ0T 5/2/ρ)τ .

χ0T 5/2
ρ(∆⋆)

∼

∆⋆2
τ

.

(III.20)

This again fixes ρ(∆⋆) (hence c) at a given s and τ (1–3 ns in our metric).

Put together, for each slope s there is a best ρ(∆⋆) that simultaneously (i) matches alpha
areal-range (ρ∆⋆ ∼ Rm) and (ii) conductive reach (ℓc ∼ ∆⋆). Because c is the degree of
freedom that reshapes ρ(∆) locally at ∆⋆ while keeping the ramp endpoint fixed, you get an
ideal curvature c⋆(s). This c⋆(s) traces the observed ridge.

Intuition from that criterion:

• If s is too shallow, ρ(∆) stays small; you need a less negative c (or even positive) to

raise ρ at ∆⋆ enough to meet ρ∆⋆ ∼ Rm.

• If s is too steep, ρ(∆) climbs quickly; you need more negative c to delay the rise so

alphas/conduction can reach and heat ahead of the front before losses spike.

The ridge curvature is thus the locus where ρ(∆⋆) hits the Goldilocks window: big enough

for Qα and confinement, small enough for reach and low radiative drain.

We also developed tests to be sure we were clear about how the ridge should shift with

physics knobs.

This picture immediately predicts how the ridge moves if you change microphysics:

• Longer alpha range Rm: pushes the optimum toward shallower s and more negative c

(to keep ρ∆⋆ ∼ Rm).

• Stronger radiation (larger Crad or lower opacity): favors shallower densities near the

front, so more negative c at a given s.

• Lower hot-spot temperature: reduces source, so the optimum shifts toward slightly larger

s (more ρ where the front forms) but still prefers negative c to preserve reach.

47

Finally, we ended with a compact “engineering rule.” Pick a target preheat distance ∆⋆
tied to your timing window (e.g., 1–3 ns) and desired 5 keV crossing. Then design (s, c) so
that

ρ(∆⋆) ∆⋆ ≈ Rm and

χ0T 5/2
ig
ρ(∆⋆)

≈

∆⋆2
τ

.

(III.21)

This reduces to a single condition on ρ(∆⋆), hence a curvature c⋆ at each slope s. In practice you
will land on shallow slopes with moderately negative curvature, exactly what the simulations
highlighted.

This formula offers a check on our results, using microphysics and power balance to estimate
the expected c⋆(s) relationship. As shown in Figure III.2, the red engineering rule and the
numerically obtained ridge are pretty close. I invested about 6 hours in this project (it is quite
interesting, but the exercise was to be quick), and I ended with both a numerical result and
a theoretical explanation that give me confidence to proceed. As I mentioned earlier, this
process itself is not novel. It is the workflow of a design physicist operating in the cutting-edge
field of ICF. However, executing this workflow from concept, to numerical exploration, to
theoretical supporting statement in hours is rather amazing.

Figure III.2: The red line represents the theoretical ridge slope developed by our theoretical
exposition. It fits reasonably well to the numerical ridge. The combination of a numerical
result verified by a theoretical power balance argument increases confidence in results.

III.3.5 Physics Conclusions and Study Outcomes

At the outset, I was looking for a set of physics goals, and we delivered:

48

• A static, 1-D spherical reaction–diffusion model captures the essential burn-wave physics

of ICF hot-spot ignition and propagation.

• Including realistic alpha stopping and nonlocal deposition with gray radiative loss yields

credible propagation thresholds (marginal ignition near 1 keV).

• The ridge analysis identifies a family of fuel ramps – shallow slope, moderately negative

curvature – that maximize front speed at fixed Ths.

• A theoretical analysis explains the ridge in density profile space from a fundamental

power balance in a co-moving wave front.

• The resulting framework serves as a physics-informed testbed for developing theory,
validating higher-fidelity hydrodynamic simulations, and designing future experiments.

This is a representative use of GPT-5 for my experience. The model took the seed of an
idea that I had – a static model to get a feel for key physics at the initiation and propagation
of a thermonuclear burn wave – and it helped me very quickly build a partial differential
equation representation of the model. On command, it incorporated detailed stopping power
and familiar (to the expert) physical data known to the ICF community. It discretized the
problem, set up the optimization routine I imagined, and delivered the result I thought we
would expect. This took me about 6 hours from start to finish – idea to writing. There is not
too much expertise that I could not find by querying my colleagues at the national labs, doing
internet search, deriving some things by hand, recalling some undergraduate and graduate
numerical tricks, and writing a few hundred lines of code. And, that is amazing. It made me
a one-person army of experts all rolled up, as if I were the best I had ever been at everything
I have ever thought about ICF. I can name the world experts it felt like I was talking to or
the ones I would have had to reach out to to help me with some small step. But, they were
right there in my laptop (more or less).

The theoretical explanation that I worked up with it is probably the best part. This
explanation directly ties known physics relations together to give a closed-form predictor of
what we had already found. This is a good way of ensuring that the results built over hours of
elicitation with the model are not wrong and that the model had not seduced me into buying
its arguments.

The results, I must say, do not come without work. The model, in its eagerness to please,
often introduces numerical duct tape to smooth over a thorny issue, silently swaps out detailed
numerical solves for approximations with trends it knows I want, and confidently declares
victory when numerical signals are still obviously noise. When pointed to these errors and
unwanted outcomes, the model is very good at actually fixing the issue. But, users must be
expert enough to catch the oversimplifications, must persist to get the model to reconsider,
and must be vigilant to ensure that the refined results are driving in the right direction.

Overall, as I have noted many times, I feel like my 6 hours of work here yielded something
I could have done over a month or two with a very good pair of postdocs – one for theory and
one for numerics. So, say 6 person-months reduced to 6 person-hours. That is a compression of
about a factor of 1000. I can only imagine what two postdocs and I could do if we collaborated
with each of us working in this AI-accelerated mode.

49

III.3.6 Part II — Final Ridge Map Code

Below is the complete Python code used to generate the Gaussian-smoothed ridge map and
extract the five density profiles.

1 import numpy as np
2 import matplotlib . pyplot as plt
3 from numpy . linalg import lstsq
4
5 # - - - - - - - - - - - - - - - - - - Physical Constants - - - - - - - - - - - - - - - - - -
6 MeV_to_J = 1.602176634 e -13
7 m_u = 1.66053906660 e -27
8 m_ion = 2.5 * m_u
9 k_B = 1.380649 e -23
10 K_per_keV = 1.1604518 e7
11 Cv = 1.5 * k_B / m_ion
12 E_fus = 17.6 * MeV_to_J
13 E_alpha = 3.5 * MeV_to_J
14 alpha_fraction = E_alpha / E_fus
15
16 # - - - - - - - - - - - - - - - - - - Regions & Profiles - - - - - - - - - - - - - - - - - -
17 class Regions :
18

def __init__ ( self ) :

19

20

21

22

23

24

self . R_hs = 50 e -6
self . L_ramp = 45 e -6
self . L_cold = 90 e -6
self . rho_hs = 1200.0
self . rho_cold = 1.5 e5
self . T_cold_keV = 0.3

32

31

30

25
26 def make_regions () : return Regions ()
27
28 def densi ty_ p rof i le_ norm (r , regs , slope , curvature ) :
R1 , R2 = regs . R_hs , regs . R_hs + regs . L_ramp
29
rho = np . empty_like ( r )
rho [ r <= R1 ] = regs . rho_hs
mask = ( r > R1 ) & ( r <= R2 )
d = r [ mask ] - R1
ramp = regs . rho_hs + slope * d + curvature *( d **2)
end0 = regs . rho_hs + slope *( R2 - R1 ) + curvature *( R2 - R1 ) **2
scale = ( regs . rho_cold - regs . rho_hs ) /( end0 - regs . rho_hs )
ramp = regs . rho_hs + scale *( ramp - regs . rho_hs )
rho [ mask ] = np . clip ( ramp , regs . rho_hs , regs . rho_cold )
rho [ r > R2 ] = regs . rho_cold
return rho

34

33

35

36

37

39

40

38

41
42 # - - - - - - - - - - - - - - - - - - Transport / Sources - - - - - - - - - - - - - - - - - -
43 def th erma l_d iffusi vity (T , rho , chi0 =0.12) :
44

return chi0 * np . maximum (1 e -6 , T ) **2.5 / np . maximum (1 e -3 , rho )

45
46 def bosch_ hale_D T_ cm3s ( T_keV ) :
47

48

49

50

51

52

53

C1 = 1.17302 e -9
C = [0.0 , C1 , 1.51361 e -2 , 7.51886 e -2 , 4.60643 e -3 , 1.35 e -2 ,

-1.0675 e -4 , 1.366 e -5]
B_G = 34.3827; mr_c2 = 1124656.0
T = np . maximum (1 e -6 , T_keV )
th = T /(1 - ( T *( C [2]+ T *( C [4]+ T * C [6]) ) ) /(1+ T *( C [3]+ T *( C [5]+ T * C [7]) ) ) )
eta = ( B_G **2/(4.0* th ) ) **(1/3)

50

54

return C [1]* th * np . sqrt ( eta /( mr_c2 * T **3) ) * np . exp ( -3* eta )

55
56 def f u s i o n_p o wer _ den sity (T , rho , burn ) :
57

sv = b osch_ ha le_DT_cm3s ( T ) *1 e -6
n_i = rho / m_ion
return 0.25* n_i **2*(1 - burn ) **2* sv * E_fus

58

59

60
61 def ra diat ion _loss_ gray (T , rho , rhoR , C_rad =3 e -36 , kappa0 =5 e -3) :
62

emiss = C_rad * rho **2* np . sqrt ( np . maximum (1 e -6 , T ) )
Pesc = 1/(1 + kappa0 * np . maximum (0 , rhoR ) )
return emiss * Pesc

63

64

65
66 # realistic alpha mass range
67 def mass_range_Rm ( T ) : return 0.30*( T /3) **0.5
68 def alpha_ range_ le ngth (T , rho ) :
69

return ( mass_range_Rm ( T ) / np . maximum (1 e -6 , rho /1000) ) *1 e -2

# g / cm ^2

70
71 def rhoR_outward (r , rho ) :
72

dr = r [1] - r [0]; rho_gcc = rho /1000
return np . cumsum ( rho_gcc [:: -1]) * dr *100.0[:: -1]

74
75 # - - - - - - - - - - - - - - - - - - Finite - Hotspot Solver - - - - - - - - - - - - - - - - - -
76 def step_CN_finite (T , B , r , rho , dt , chi0 =0.12) :
77

N = len ( T ) ; dr = r [1] - r [0]
chi = th e rm a l_d iffusivity (T , rho , chi0 )
chi_i = 0.5*( chi [: -1]+ chi [1:])
a = np . zeros (N -1) ; b = np . zeros ( N ) ; c = np . zeros (N -1)
for i in range (1 ,N -1) :

Aip =(( r [ i +1]+ r [ i ]) /2) **2* chi_i [ i ]/ dr
Aim =(( r [ i ]+ r [i -1]) /2) **2* chi_i [i -1]/ dr
vol = r [ i ]**2; a [i -1]= Aim / vol ; c [ i ]= Aip / vol ; b [ i ]= -( Aim + Aip ) / vol

b [0]= -2* chi_i [0]/ dr **2; b [ -1]= -2* chi_i [ -1]/ dr **2
rhs = T + dt *(( fu sion_power _d en si ty (T , rho , B ) * alpha_fraction

- radiation_loss _gray (T , rho ,0) ) / ( Cv * rho * K_per_keV ) )

# simple implicit diffusion
M = np . zeros (( N , N ) ) ; np . fill_diagonal (M ,1 - dt * b )
for i in range (N -1) : M [i , i +1]= - dt * c [ i ]; M [ i +1 , i ]= - dt * a [ i ]
Tnew = np . linalg . solve (M , rhs )
return np . clip ( Tnew ,0 ,400) ,B

93
94 def run_sim_finite ( regs , slope , curv , Ths , t_end =3.5 e -9 , N =150 , Nt =140) :
95

Rtot = regs . R_hs + regs . L_ramp + regs . L_cold
r = np . linspace (0 , Rtot , N )
rho = den s ity _ pro f ile_norm (r , regs , slope , curv )
T = np . full_like (r , regs . T_cold_keV ) ; T [r <= regs . R_hs ]= Ths
B = np . zeros_like ( T )
times = np . linspace (0 , t_end , Nt +1) ; dt = t_end / Nt
Th = np . zeros (( Nt +1 , N ) ) ; Th [0]= T
for n in range (1 , Nt +1) :

T , B = step_CN_finite (T ,B ,r , rho , dt )
Th [ n ]= T

class Result : pass
res = Result () ; res . r = r ; res . times = times ; res . T_hist = Th ; res . rho = rho ; res .
meta ={ ’ regs ’: regs }
return res

108
109 # - - - - - - - - - - - - - - - - - - Speed Metric - - - - - - - - - - - - - - - - - -

51

73

78

79

80

81

82

83

84

85

86

87

88

89

90

91

92

96

97

98

99

100

101

102

103

104

105

106

107

110 def interp_front (r ,T , Tthr =5.0) :
111

idx = np . where (T >= Tthr ) [0]
if len ( idx ) ==0: return np . nan
i2 = idx [ -1]; i1 = max ( i2 -1 ,0)
T1 , T2 = T [ i1 ] , T [ i2 ]; r1 , r2 = r [ i1 ] , r [ i2 ]
return r1 +( Tthr - T1 ) /( T2 - T1 +1 e -12) *( r2 - r1 )

116
117 def mean_speed ( res , Tthr =5.0 , t0 =1 , t1 =3) :
118

t = res . times *1 e9 ; r = res . r
mask =( t >= t0 ) &( t <= t1 )
rf =[ interp_front (r , res . T_hist [ i ] , Tthr ) for i in np . where ( mask ) [0]]
rf = np . array ( rf ) ; ok =~ np . isnan ( rf )
if ok . sum () <3: return 0
A = np . vstack ([ t [ mask ][ ok ]*1 e -9 , np . ones ( ok . sum () ) ]) . T
v , _ = lstsq (A , rf [ ok ] , rcond = None ) [0]
return max (0 , v )

112

113

114

115

119

120

121

122

123

124

125

126
127 # - - - - - - - - - - - - - - - - - - Ridge Map Generation - - - - - - - - - - - - - - - - - -
128 regs = make_regions () ; T_hs =1.5
129 slopes = np . linspace (1.5 e9 ,6.5 e9 ,11)
130 curvs = np . linspace ( -6.5 e13 ,0 ,11)
131 V = np . zeros (( len ( slopes ) , len ( curvs ) ) )
132 for i , s in enumerate ( slopes ) :
133

for j , c in enumerate ( curvs ) :

134

135

res = run_sim_finite ( regs ,s ,c , T_hs )
V [i , j ]= mean_speed ( res )

136
137 # Gaussian kernel smoother
138 def gauss_smooth (V , sigma =1) :
139

140

from scipy . ndimage import gaussian_filter
return gaussian_filter (V , sigma = sigma , mode = ’ nearest ’)

141
142 Vg = gauss_smooth (V ,1)
143
144 # Ridge extraction
145 ridge_s =[]; ridge_c =[]
146 for j in range ( Vg . shape [1]) :
147
148 ridge_s = np . array ( ridge_s ) ; ridge_c = np . array ( ridge_c )
149
150 # Ridge plot
151 plt . figure ( figsize =(8 ,5) )
152 im = plt . imshow ( Vg .T , origin = ’ lower ’ ,
153

i = np . argmax ( Vg [: , j ]) ; ridge_s . append ( slopes [ i ]) ; ridge_c . append ( curvs [ j ])

extent =[ slopes . min () , slopes . max () , curvs . min () , curvs . max () ] ,
aspect = ’ auto ’)
154
155 plt . colorbar ( im , label = ’ mean front speed ( m / s ) ’)
156 plt . plot ( ridge_s , ridge_c , ’k - - ’ , linewidth =2 , label = ’ ridge ’)
157 plt . xlabel ( ’ slope ( kg / m ^4) ’) ; plt . ylabel ( ’ curvature ( kg / m ^5) ’)
158 plt . legend () ; plt . tight_layout ()
159 plt . show ()

Listing III.1: Ridge map generation and profile extraction.

52

Chapter IV

New scientific results obtained with
AI

IV.1 Erdős problems (part 2/2) – Mehtaab Sawhney and Mark

Sellke

We continue from where we left off in Section II.2 and explain how GPT-5 Pro assisted us in
solving Erdős Problem #848.

IV.1.1 AI–assisted solution to Problem #848

Erdős Problem #848 is in combinatorial number theory, and was posted as Problem 23 in
[Erd92]. This problem was genuinely open, and has now been solved by the first author and
GPT-5, in combination with online comments of van Doorn, Weisenberg and Cambie. The
statement is:

Problem #848: Let A be a subset {1, 2, . . . , N } such that ab + 1 is not squarefree for all
a, b ∈ A (i.e. ab + 1 must be divisble by p2 for at least one prime p). Then for all sufficiently
large N , no such set A has more elements than A = {1 ≤ a ≤ N : a ≡ 7 (mod 25)}.

Of course, the given set A satisfies the desired property because 72 + 1 ≡ 0 (mod 25). It
further turns out that this maximal A is unique except when {1 ≤ a ≤ N : a ≡ 18 (mod 25)}
has the same size, and for some ε > 0 any A containing at least (1−ε)N
elements must be
contained in one of these two. A full solution of this problem is included in the appendix.

25

This question is in a line of extremal questions raised by Erdős at the intersection of
elementary number theory and combinatorics. For instance, Erdős and Silverman asked what
is the densest subset of [N ] such that A + A avoids a perfect square; this was solved by [KLS02]
who showed that |A| ≤ (11/32 + o(1))N . Erdős similarly asked (also within Problem 23 of
[Erd92]) about the largest collection A ⊆ [N ] such that ab is never square–free.

The starting point of the solution is to observe that the diagonal constraint that a2 + 1 not
be squarefree already places a substantial restriction on A. Indeed, classical number theory
implies that only primes p ≡ 1 (mod 4) can divide a2 + 1 twice, and for each such prime only
2/p2 fraction of integers a have a2 + 1 divisible by p. Since divisibility by powers of different

53

primes behaves approximately independently, this already implies the simple upper bound

1 − Y

p prime,
p≡1(mod 4)

(cid:16)

1 −

(cid:17)

2
p2

≈ 0.105

for the density of A, which is within a factor of 3 of the truth.

What is more challenging is to use the off-diagonal constraints on ab + 1 to non-trivially
improve this upper bound. For this the first author asked GPT-5 Pro for suggestions, via the
prompt displayed in Figure IV.1 below (which itself built on online comments of van Doorn,
Weisenberg and Cambie).

The idea suggested by GPT-5’s reply in Figure IV.1 gives a method to use any single
number b ∈ A to obtain similarly harsh constraints on all other a ∈ A. For example, let A∗
be the subset of A consisting of numbers congruent to neither 7 nor 18 mod 25. If A∗ is
non-empty, then fixing any b ∈ A∗, we have for any a ∈ A that both a2 + 1 or ab + 1 must
be divisible by p2 for some prime p. But since b ∈ A∗ it follows that only one of them can
be divisible by 5 itself. This is a much stronger restriction on a ∈ A and already yields the
improved bound:

(cid:17)(cid:16)

(cid:16) 23
25

1 − Y
p≡1

(mod 4)

(cid:16)

1 −

(cid:17)(cid:17)

+

2
p2

(cid:16)

2
25

1 − Y
p(cid:44)5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

p≥13

≤ .02517 + .0294 ≈ .05457.

Indeed for the (23/25)-fraction of integers not congruent to 7 or 18 mod 25 we apply the
diagonal argument above, and if a ≡ 7, 18 (mod 25) then ab + 1 must be divisible by p2 for
p (cid:44) 5. This bound is already not far from the truth, which suggests that pushing it further
ought to solve the entire problem. GPT-5 made attempts in this direction but had numerous
errors in its implementation (as can be seen in the transcript). Still, the high-level idea was
sound and was quickly turned into a complete proof.

A first step of the eventual solution was to note that if b ∈ A∗ is moreover even, then ab + 1

cannot be divisible by 4 and so the bound above improves to

(cid:17)(cid:16)

(cid:16) 23
25

1 − Y
p≡1

(mod 4)

(1 −

(cid:17)

2
p2 )

+

(cid:16)

2
25

1 − Y
p(cid:60){2,5}

(1 −

1
(cid:17)
p2 )

p≥13

≤ .02517 + .0125 ≈ .03767.

Since .03767 < 1/25 is strictly smaller than the density of numbers 7 mod 25, this already
implies that a near-maximal set A cannot contain any even numbers in A∗ at all. Further
casework in this vein allows one to rule out the existence of any b ∈ A∗ whatsoever, and finally
to show that mixing between 7 and 18 mod 25 is inefficient as well. The main discussion with
GPT-5 can be found here: [Saw25b].

We emphasize that the first observation mentioned is due to Wouter van Doorn and the
particular prompt given to GPT-5 was based on a comment of Stijn Cambie; these were already
present in the comments of the official website https://www.erdosproblems.com/forum/
thread/848. Thus, the final proof consists of a key solution step due to GPT-5 sandwiched
between two layers of human mathematics!

54

Consider the following problem. My suspicion is that there is a
more effective way to run the inclusion–exclusion argument. The
key point is that if a number satisfies both the congruence modulo 5
and the congruence modulo 13, then the standard upper bound will
overcount, since such a number would also need an additional prime
factor (distinct from 5 and 13). My intuition is that a stability-style
analysis should help control this overcount, leading to a sharper
bound, and potentially to a full proof. Please attempt to develop a
proof along these lines.

[Reasoned for 21m 22s]
Below I’ll try to push the “necessary–condition + stability” approach you suggested
far enough to reach the natural extremal answer.

Problem
Let A ⊂ {1, . . . , N } and assume

(∀a, b ∈ A)

ab + 1 is not squarefree.

(P)

Erdős asked whether the extremal sets are the residue class

A = {n ≤ N : n ≡ 7

(mod 25)}

(or symmetrically n ≡ 18 (mod 25)), which have density 1/25.
1. The “one-variable necessary condition” (and the sharp constant) Setting
b = a in (P) shows that for every a ∈ A there is a prime p ≡ 1 (mod 4) with p2 | a2 + 1.
For such a prime p there are exactly two residues r (mod p2) with r2 ≡ −1, so the set
of integers n with p2 | n2 + 1 has density 2/p2. These events are independent across
distinct primes (CRT), hence the exact density of integers n for which p2 | n2 + 1 for

some p ≡ 1 (mod 4) is

1 −

Y

(cid:18)

1 −

p≡1

(mod 4)

(cid:19)

2
p2

= 0.10515 . . .

Consequently

|A| ≤ (0.10515 + o(1)) N.

This is the sharp inclusion–exclusion bound behind the “0.108” in the notes.
This bound, however, is only a necessary condition extracted from the diagonal pairs
(a, a). It ignores the pairwise demand in (P). The key is to exploit that demand via a
stability argument.

Figure IV.1: The start of the interaction with GPT-5 Pro to derive the improved bound
for Problem #848. In the above, it has just summarized the problem and the argument on
https://www.erdosproblems.com. Immediately after this, GPT-5 put forward the new idea
that led to our solution (see next page). The full transcript can be found here: [Saw25b].

55

2. A uniform squarefree sieve in a fixed 25-class
Fix a ≥ 1. Let t ∈ {0, . . . , 24} with t (cid:46) −a−1 (mod 25) (so that 25 ∤ ab + 1 for every
b ≡ t (mod 25)). Consider

Ba(X) := #{ b ≤ X : b ≡ t

(mod 25), ab + 1 is squarefree }.

For any odd prime p (cid:44) 5 with p ∤ a the congruence p2 | ab + 1 cuts out exactly one
residue class modulo p2; when p | a it cuts out no class. By the Chinese Remainder
Theorem, for any finite set S of such primes we have the exact local density

#{ b ≤ X : b ≡ t

(mod 25), p2 ∤ ab + 1 ∀p ∈ S }

X/25

(cid:18)

1 −

(cid:19)

1
p2

+ O

(cid:18) 1
X

(cid:19)

.

=

Y

p∈S
p(cid:44)5

Letting S grow and using convergence of Q
asymptotic:

p(1 − 1/p2), we obtain a uniform (in a)

Lemma 1 (squarefree density in a fixed 25-class).

For any fixed a and any t (cid:46) −a−1 (mod 25),

Ba(X)
X/25

=

(cid:18)

1 −

(cid:19)

1
p2

Y

p(cid:44)5

+ o(1) ≥

as X → ∞. The rightmost constant is

(cid:18)

Y

1 −

p(cid:44)5

(cid:19)

1
p2

+ o(1),

δ :=

(cid:18)

Y

p(cid:44)5

1 −

(cid:19)

1
p2

=

1
ζ(2)

·

1
1 − 1/25

=

25
4π2 = 0.633257 . . .

Thus, for every fixed a and every 25-class t different from the unique class that makes
25 | ab + 1, at least a δ − o(1) fraction of b in that class give ab + 1 squarefree.
Equivalently, at most a fraction

1 − δ = 1 −

25
4π2 = 0.366742 . . .

of those b can be “rescued” by primes other than 5.
This one fact drives the whole stability argument.

Figure IV.2: A continuation of the interaction with GPT-5 Pro to derive the improved bound.
Here it proposes the main new idea that led to the solution presented in Subsection IV.1.3.
Namely if S contains a single “unexpected” number, this can be used to strongly constrain all
other numbers in S. Note that the roles of a and b are swapped compared to Subsection IV.1.3.
The full transcript can be found here: [Saw25b].

56

IV.1.2 Looking Forward

Our investigation of Erdős problem #848 highlights that GPT-5 has the ability to serve as an
effective mathematical assistant, capable of recalling relevant lemmas, identifying analogies
and locating relevant results from vague, ill-specified prompts. However, current models
remain limited in perceiving the “negative space” of mathematics. While models are able to
suggest plausible proof strategies, they often do not realize certain “obvious” examples which
block progress, and are overly confident in the power of existing methods. This is unsurprising
as such discussion is largely absent in mathematical literature itself; mathematicians don’t
systematically record why problems are out reach, or why a more obvious strategy doesn’t
work, or why certain techniques are inherently unable to solve certain classes of problems.

IV.1.3 Formal solution of Problem #848

Proposition IV.1.1. There exists an integer N0 such that if N ≥ N0 then the following
holds. Let A ⊆ [N ] with the property that if a, b ∈ A then ab + 1 is not squarefree. Then

|A| ≤ |{n ∈ [N ] : n ≡ 7

(mod 25)}|,

i.e. {n ∈ [N ] : n ≡ 7 (mod 25)} achieves the maximum possible value of |A|.

Remark IV.1.1. Our proof actually allows one to go further and characterize all approxi-
mately maximal sets A. Namely equality is achieved precisely when A = {n ∈ [N ] : n ≡ 7
(mod 25)} or A = {n ∈ [N ] : n ≡ 18 (mod 25)} (the latter may be optimal or suboptimal by 1
depending on the value of n modulo 25). Further it turns out that for all |A| ≥ 0.038 · N within
5% of optimality, when N is sufficiently large, A must be contained in either {n ∈ [N ] : n ≡ 7
(mod 25)} or {n ∈ [N ] : n ≡ 18 (mod 25)} (and of course all such A satisfy the squarefree
condition).

We first require the following pair of preliminary lemmas; the first was already used by
van Doorn (see [Blo]). In order to be self-contained, we provide proofs with suboptimal error
terms.

Lemma IV.1.1. Let P be a subset of primes with max P ≤ N 1/2 and let q be a positive
integer. For each p ∈ P, define Rp to denote a set of residue classes modulo p2 with |Rp| ≤ 2
and Rp = ∅ if (p, q) (cid:44) 1. Then

(cid:12)
(cid:16)(cid:8)n ∈ [N ] : n ≡ t mod q(cid:9) \ [
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ O(N (log N )−1/2).

p∈P

Proof. Let T = ⌊

√

log N ⌋. Note that

{n (mod p2) ∈ Rp}

(cid:17)(cid:12)
(cid:12)
(cid:12) −

(cid:16)

N
q

1 − Y
p∈P

(cid:16)

1 −

|Rp|
p2

(cid:12)
(cid:12)
(cid:17)(cid:17)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)[N ] \ [

p∈P
T ≤p≤N 1/2

{n (mod p2) ∈ Rp}

(cid:16) X

(cid:12)
(cid:12)
(cid:12) ≤ O

T ≤p≤N 1/2

(cid:17)

N
p2

≤ O

(cid:17)

.

(cid:16) N
T

Therefore it suffices to estimate

(cid:12)
(cid:12)
(cid:12)

(cid:16)(cid:8)n ∈ [N ] : n ≡ t mod q(cid:9) \ [

p∈P
p≤T

57

{n (mod p2) ∈ Rp}

(cid:17)(cid:12)
(cid:12)
(cid:12).

Note that as T ≤

√

log N , we have that Q

p≤T p2 ≤ N o(1). Via inclusion exclusion, we have

(cid:12)
(cid:12)
(cid:12)

(cid:16)(cid:8)n ∈ [N ] : n ≡ t mod q(cid:9) \ [

{n (mod p2) ∈ Rp}

p∈P
p≤T

(cid:17)(cid:12)
(cid:12)
(cid:12)

= X

(−1)|S|−1 ·

S⊆P∩[T ]
S(cid:44)∅

(cid:12)
(cid:12)
(cid:12)

\

p∈S

{n (mod p2) ∈ Rp} ∩ {n ∈ [N ] : n ≡ t

(mod q)}

(cid:12)
(cid:12)
(cid:12)

= N o(1) + X

(−1)|S|−1 ·

N
q

·

Q

p∈S |Rp|
Q
p∈S p2

S⊆P∩[T ]
S(cid:44)∅
1 − Y

p∈P∩[T ]

=

(cid:16)

·

N
q

(cid:16)

1 −

(cid:17)(cid:17)

|Rp|
p2

+ N o(1).

Completing the product, we have the desired result.

□

We next require a variant of the previous lemma which allows us to incorporate the

“off-diagonal” constraints that ab + 1 is not squarefree.

Lemma IV.1.2. Fix a positive integer N , residue class t (mod q) with q a perfect square and
an integer b such that 1 ≤ b ≤ N . Suppose that there does not exist a prime p such that p2|q
and p2|(bt + 1).

Then we have that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12){a ∈ [N ] : a ≡ t
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) −
(mod q) ∧ µ(ab + 1) = 0}

(cid:16)

·

N
q

1 − Y
p
(p,qb)=1

(cid:16)

1 −

1
p2

(cid:17)(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ O(

√

N
log N

).

Proof. Note that ab + 1 ≤ N 2 + 1. Therefore if µ(ab + 1) = 0, there exists a prime p such
that p2|(ab + 1). Additionally note that (p, qb) = 1. In particular, if p|b then p ∤ ab + 1.
Furthermore if p|q, then observe ab + 1 ≡ bt + 1 (cid:46) 0 (mod p2) by the imposed condition.

We now let T = ⌊

log N ⌋. We have that

√

X

T ≤p≤N

|{a ∈ [N ] : p2|(ab + 1)}| ≤ O

(cid:16) X

T ≤p≤N

(cid:17)(cid:17)

(cid:16) N
p2 + 1

≤ O

(cid:17)

.

(cid:16) N
T

Therefore it suffices to handle p ≤ T with (p, qb) = 1. The condition that p2|ab + 1 gives one
specified residue class (depending on each prime p) and therefore we may handle the remaining
□
primes via Lemma IV.1.1 and we may conclude the proof.

We now give the remainder of the analysis to conclude the proof. The proof consists of
breaking the set into the parts which are 7 (mod 25), those that are 18 (mod 25), and all
the rest. We then apply combinations of Lemma IV.1.1 and Lemma IV.1.2 based on whether
the relevant sets are nonempty. To optimize the numerical factor of Q
(p,qb)=1(1 − 1/p2), we
introduce casework based on whether these sets contain an even integer or consist solely of
odd numbers. We note that there exist other similar solutions within this framework; indeed
our initial version of the solution required more extensive casework modulo numbers including
169 and 289. The conversation with GPT-5 [Saw25b] in particular uses the auxiliary prime 3
and therefore the precise computation differs from the draft here.

58

Proof of Proposition IV.1.1. Let η > 0 be a small absolute constant (in fact η = 0.002 suffices),
and let N be sufficiently large and suppose that |A| ≥ (1/25 − η) · N . Define

A7 = {a ∈ A : a ≡ 7
A18 = {a ∈ A : a ≡ 18
A∗ = A \ (A7 ∪ A18).

(mod 25)}

(mod 25)}

We first prove that A∗ is empty.
Suppose that there exists b ∈ A∗ with 2|b. Then for each x ∈ A∗, there is p ≡ 1 (mod 4)
with p ≥ 13 such that p2|x2 + 1. By breaking A∗ into residue classes modulo 25 (of which
there are 23 possibilities) and applying Lemma IV.1.1, we have that

|A∗|
N

≤

(cid:16) 23
25

(cid:17)(cid:16)

1 − Y

(cid:16)

1 −

(cid:17)(cid:17)

+ o(1).

2
p2

p≡1 (mod 4)
p≥13
For a ∈ A7 ∪ A18, there exists p (cid:44) 2, 5 such that p2|ab + 1. This implies by Lemma IV.1.2 that

|A7 ∪ A18|
N

≤

(cid:16) 2
25

(cid:17)(cid:16)

1 − Y
p(cid:44)2,5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1).

Combining these bounds we have

|A|
N

≤

(cid:16) 23
25

(cid:17)(cid:16)

1 − Y
p≡1

(mod 4)

(cid:16)

1 −

(cid:17)(cid:17)

+

2
p2

(cid:16)

2
25

1 − Y
p(cid:44)2,5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1)

p≥13

≤ 0.0252 + 0.0125 = 0.0377 < 0.04 − η;

here we have absorbed the o(1) error assuming that N is sufficiently large. This contradicts
our assumption that |A| ≥ (1/25 − η)N .

We have now reduced to the case that A∗ consists only of odd elements. By breaking
A∗ into residue classes modulo 50 and noting there are 23 valid possibilities and applying
Lemma IV.1.1, we have that

|A∗|
N

≤

(cid:16) 1
2

(cid:17)(cid:16) 23
25

(cid:17)(cid:16)

1 − Y
p≡1

(mod 4)

p≥13

(cid:16)

1 −

(cid:17)(cid:17)

2
p2

+ o(1).

We now split further into cases based on whether or not A7 ∪ A18 contains even elements. First
suppose that no element of A7 ∪ A18 is even; then A7 ⊆ {7, 57} (mod 100) and A18 ⊆ {43, 93}
(mod 100). Fixing b ∈ A∗, for each of A7 and A18 one of the residue classes
(mod 100)
will have 22 as a divisor and the other will not. Therefore applying Lemma IV.1.2 to these
progressions (mod 100), we have

|A7 ∪ A18|
N

≤

1
50

+

(cid:16) 1
50

(cid:17)(cid:16)

1 − Y
p(cid:44)2,5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1).

This gives another contradiction as

|A|
N

≤

(cid:16) 1
2

(cid:17)(cid:16) 23
25

(cid:17)(cid:16)

1 − Y
p≡1

(mod 4)

p≥13

(cid:16)

1 −

(cid:17)(cid:17)

+

2
p2

1
50

+

(cid:16) 1
50

(cid:17)(cid:16)

1 − Y
p(cid:44)2,5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1)

≤ 0.0126 + 0.0200 + 0.0032 = 0.0358 < 0.04 − η.

59

Therefore we have reduced to the case where A∗ is nonempty, consisting of odd elements and
at least one of A7 or A18 has an even element. We now fix b ∈ A∗ and b′ ∈ A7 with 2|b′ (with
the case where A18 has the even element being completely analogous). The size of |A∗| is
bounded as before. By using b and Lemma IV.1.2, we have that

|A7|
N

≤

(cid:16)

1
25

1 − Y
p(cid:44)5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1).

Additionally, using b′ ∈ A7 and Lemma IV.1.2, we have that

|A18|
N

≤

(cid:16)

1
25

1 − Y
p(cid:44)2,5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1).

Therefore we have that

|A|
N

≤

(cid:16) 1
2

(cid:17)(cid:16) 23
25

(cid:17)(cid:16)

1 − Y

p≡1 (mod 4)
p≥13

(cid:16)

1 −

(cid:17)(cid:17)

+

2
p2

(cid:16)

1
25

1 − Y
p(cid:44)5

(cid:16)

1 −

(cid:17)(cid:17)

+

1
p2

(cid:16)

1
25

1 − Y
p(cid:44)2,5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1)

≤ 0.0126 + 0.0147 + 0.0063 = 0.0336 < 0.04 − η.

Therefore we have now reduced to the case where A∗ = ∅. Suppose that A7 and A18 are

nonempty; fixing b ∈ A7 and b′ ∈ A18 and applying Lemma IV.1.2 twice we have that

|A|
N

≤

(cid:16)

2
25

1 − Y
p(cid:44)5

(cid:16)

1 −

(cid:17)(cid:17)

1
p2

+ o(1) ≤ 0.0294 < 0.04 − η.

Therefore A is fully contained inside one of A7 or A18. This immediately gives the desired
□
bound.

We record various numerical constants to 4 digits, rounding to preserve the stated inequal-

ities.

(cid:16)

• Q
p

1 − 1
p2

• Q
p(cid:44)5

(cid:16)

1 − 1
p2

(cid:17)

(cid:17)

= 6

π2 ≥ 0.6079, so 1 − 6

π2 ≤ 0.3921.

= 25

4π2 ≥ 0.6332, so 1 − 25

4π2 ≤ 0.3668.

(cid:16)

• Q
p(cid:44)2,5

(cid:17)

1 − 1
p2

= 25

3π2 ≥ 0.8443, so 1 − 25

3π2 ≤ 0.1557.

• 1 − Q

(cid:16)

p≡1 (4)
p≥13

(cid:17)

1 − 2
p2

≤ 0.0274.

60

IV.2 New online algorithms lower bounds – Christian Coester

This section describes how GPT-5 assisted in proving new bounds on the performance of
algorithms for a fundamental problem in online optimization called convex body chasing. The
problem was introduced by [FL93] and captures how to make sequential decisions under
uncertainty when each step restricts feasible options in a geometric way. Formally, one is
given a sequence of convex sets K1, K2, . . . , KT ⊂ Rd, revealed one by one. Upon seeing
Kt, an algorithm must choose a point pt ∈ Kt before learning the next set. The goal is to
minimize the total movement PT
t=1 ∥pt − pt−1∥. The algorithm is called online as it has to
make decisions on the fly, without knowledge of the future. In contrast, an offline algorithm is
one that knows the entire input upfront, allowing it to move optimally. The standard measure
of performance of an online algorithm is its competitive ratio, defined as the worst-case (i.e.,
maximum) ratio between the online algorithm’s cost and the optimum (offline) cost on any
instance.

Intuitively, convex body chasing formalizes the tension between adaptivity and stability:
each new convex body represents a changing feasible region (e.g., updated constraints, new
data, or evolving system state), and the online algorithm must adjust with minimal movement.
It arises naturally in contexts like control theory (tracking feasible states), machine learning
(online convex optimization with movement costs), and operations research (dynamic resource
allocation). In theoretical computer science, it serves as a clean abstraction for understanding
how well an online decision process can perform relative to an omniscient optimum, with close
connections to other central problems in online optimization [BC22].

√

When introducing the problem, [FL93] showed that the competitive ratio of any online algo-
rithm is at least
d in d-dimensional Euclidean space. Following a sequence of breakthroughs
in recent years ([Bub+19; Arg+21; Sel23]), it is now established that the optimal competitive
ratio lies in the interval [
d, d]. An important special case that received significant attention
is chasing nested convex bodies, where K1 ⊃ K2 ⊃ · · · ⊃ KT ([Ban+20; Arg+19; Bub+20]).
The following sections show how GPT-5 refuted competitiveness of a natural algorithm in
response to a single prompt, and how it assisted in raising the lower bound on the competitive
d to π
ratio of arbitrary online algorithms from
2

⌊ d
2 ⌋ ≈ 1.11

q

√

√

√

d.

IV.2.1 Lower bound for Follow-the-Leader

When the initial location is p0 = 0, observe that for nested convex body chasing, the optimal
offline algorithm will move to arg minp∈KT ∥p∥, which is feasible for all time steps and minimizes
cost. So a natural online algorithm is to move to pt = arg minp∈Kt ∥p∥ at each time t, i.e.,
tracking the currently optimal offline location. I did not necessarily expect this algorithm to
perform well as this type of follow-the-leader algorithm is known to have poor competitive
ratio for related problems, and the reason is typically trivial. However, I could not see any
such easy argument to refute the algorithm, and what made me curious is that it does achieve
the optimal competitive ratio of π/2 on the specific hard instance described in Section IV.2.2
that yields a π/2 lower bound for arbitrary algorithms in the two-dimensional case. Given
a single short prompt, GPT-5 produced a rather non-obvious counter-example against the
algorithm. It shows that the cost of the algorithm can be made arbitrarily large even in the
two-dimensional nested case when the initial body is the unit ball around the start location
(and thus proves that the competitive ratio is infinite, since the optimal offline cost is at most
1 here):

61

Consider nested convex body chasing in 2-dimensional Euclidean
space, where the initial location is 0 and the initial convex body
is the unit ball centered at 0. Prove or disprove that the cost of
the follow-the-leader algorithm (i.e., always go to the point in the
current convex body whose norm is minimal) is at most pi/2.

[Reasoned for 16m 19s]
Short answer: the statement is false.
[...]

1) Two basic facts about the greedy minimizer
Let K0 ⊇ K1 ⊇ K2 ⊇ · · · ⊂ R2 be a nested sequence of nonempty closed convex sets,
with K0 the closed unit disk centered at 0.
Let pt ∈ Kt be the unique point minimizing ∥x∥ over Kt (uniqueness follows from
strict convexity of the Euclidean norm and convexity of Kt).
Write pt = rtut with rt = ∥pt∥ ∈ [0, 1] and ut ∈ S1.
Two standard properties hold:

1. Half-space support: for every t,

Kt ⊆ H(pt) := {x : ⟨pt, x⟩ ≥ r2

t }.

Equivalently, for all s ≥ t,

⟨pt, ps⟩ ≥ r2

t ⇐⇒ cos(∠(ut, us)) ≥

rt
rs

.

(1)

(This is just the normal cone condition at a Euclidean projection.)

2. Realizability: conversely, if a sequence (pt) with nondecreasing radii rt satisfies

(1) for all s ≥ t, then setting

Kt := B(0, 1) ∩

H(pi)

\

i≤t

yields a nested sequence for which the unique minimum-norm point in Kt is
exactly pt. (Indeed, pt ∈ Kt by (1), and H(pt) excludes every point of norm
< rt; convexity then forces uniqueness.)

Thus, to disprove any universal upper bound on the greedy cost, it suffices to exhibit a
sequence (pt) ⊂ B(0, 1) satisfying (1) whose polygonal length P
t ∥pt+1 − pt∥ exceeds
that bound.

62

2) An alternating construction

Pick a nonincreasing sequence of angles {φm}m≥1 with

0 < φm+1 ≤ φm <

π
8

,

∞
X

m=1

φm = ∞,

∞
X

m=1

φ2

m < ∞.

(For instance, φm = c
m with a small fixed c ∈ (0, π/8) works.)
Choose a small r1 > 0. Define a sequence of points by alternating signs and increasing
radii:

p2m−1 = r2m−1 (cos(−φm), sin(−φm)),
p2m = r2m (cos( φm), sin( φm)),

with radii chosen recursively so that the immediately preceding constraint in (1) is
tight:

r2m =

r2m−1
cos(φm + φm)

=

r2m−1
cos(2φm)

,

r2m+1 =

r2m
cos(φm + φm+1)

.

(2)

(Geometrically, each new point lies on the boundary of the supporting half-space
defined by the previous minimizer, with the largest possible angular change allowed
by that one constraint.)
[...]

H(p2)

H(p4)

H(p1)

H(p3)

p2 p4p6

0

p1 p3p5

Figure IV.3: Beginning of a hard instance for the follow-the-leader algorithm. Figure made by
GPT-5.

63

Figure IV.3 contains an illustration of the construction. GPT goes on to justify the
feasibility of the construction: “We must verify that (1) holds not only for the immediate
predecessor but for all earlier indices.” The argument it gives for this is actually incorrect,
but a correct argument is easy to see: For s ≥ t + 1, we obtain (1) from the following chain of
inequalities:

cos(∠(ut, us)) ≥ cos(∠(ut, ut+1)) =

rt
rt+1

≥

rt
rs

.

GPT completes the proof (within the same response) as shown in the next chat box. The
first line in it contains a slight inaccuracy, since we need the inequality not only for |x| ≤ π/8
but for |x| ≤ π/4. It holds even in that case though, so the overall proof is not affected.

Finally, because P φ2
stays bounded:

m < ∞ and 1/ cos x ≤ exp(cid:0)x2(cid:1) for |x| ≤ π/8, the product in (2)
(cid:17)

(cid:16)

rt ≤ r1 · exp

C

φ2
m

< ∞.

sup
t

X

m≥1

Choosing r1 > 0 small enough we ensure rt ≤ 1 for all t, so every pt lies in the unit
disk.

3) The cost explodes

The distance traveled in the two types of steps is explicit, because in (2) the immediately
previous constraint is tight:

• Flip step p2m−1 → p2m (angles −φm → +φm):

∥p2m − p2m−1∥ =

q

2m − r2
r2

2m−1 = r2m−1 tan(2φm).

• Cross step p2m → p2m+1 (angles +φm → −φm+1):

∥p2m+1 − p2m∥ =

q

2m+1 − r2
r2

2m = r2m tan(φm + φm+1).

Therefore, after N pairs the greedy cost satisfies

LN =

≥

N
X

(cid:16)

m=1

N
X

(cid:16)

m=1

r2m−1 tan(2φm) + r2m tan(φm + φm+1)

(cid:17)

2r2m−1φm + r2m(φm + φm+1)

(cid:17)

(since tan x ≥ x for x ∈ [0, π/2])

≥ c

N
X

m=1

φm

for some constant c > 0 depending only on r1 and {φm} (because the radii are
nondecreasing [...]). Since P

m φm = ∞, we obtain LN → ∞ as N → ∞. [...]

Thus the “follow-the-leader” cost can be made arbitrarily large while all requests stay nested
inside the unit disk. The original interaction is available at [Coe25a].

64

IV.2.2 Improved lower bounds for general algorithms

We now discuss how GPT-5’s assistance helped in proving a better lower bound on the
competitive ratio of arbitrary online algorithms. The idea for this lower bound is based on a
previous 1.5358 lower bound due to [Bie+19] for chasing lines (i.e., where each Kt is a line),
which was the only previous known improvement over the aforementioned
d (when d = 2).
The proof in [Bie+19] is rather ad-hoc, giving a small number of request lines issued in three
steps, using six rational constants to specify those lines. It was acknowledged by [Bie+19]
that 1.5358 does not seem to be tight, since a generalization of their idea, using more steps
and request lines, should achieve a better bound. However, this would make their analysis
even more involved.

√

My goal was to derive a better and cleaner lower bound using a generalization of their
approach where the request line moves continuously. My initial attempts to prompt GPT-5 to
help with this construction were unsuccessful: It only reproved the known 1.5358 lower bound,
failing to understand why my suggested continuous generalization would be helpful. Curiously
though, the reasoning trace preview mentioned at one point that it was aiming for a lower
bound of π/2 ≈ 1.5708, a quantity I had not thought of before (the prompt of Section IV.2.1
took place at a later time). Although the reasoning trace preview did not substantiate why it
would aim for this number specifically, and the responses did not mention it any more, it may
have contributed to inspiring me towards the following, more concrete plan for a lower bound:

Lower bound idea (see Figure IV.4). Consider an instance with initial location (0, 0),
each request set is a line, initially the line y = 0. Start rotating the line clockwise around
B = (1, 0). Observe that the projection pt of the origin onto the current line Kt is the north-
western intersection of Kt with the circle of radius 1/2 around (1/2, 0). This is essentially due
to Thales’ theorem. If the online algorithm follows this curve until the request line is vertical,
then it moves along the semicircle and its cost is exactly π/2 whereas the optimal cost is 1.
So an algorithm has to deviate from this curve in order to achieve a smaller competitive ratio.
In fact, since we can force the algorithm to end at B, it has to go “below” the semicircle
curve at some point, otherwise its path is even longer. Consider the first time t when the
online algorithm moves “below” the semicircle curve. Let B′ be the intersection other than B
between Kt and the circle of radius 1 around the origin. Observe that pt is the midpoint of the
line segment BB′, and the minimal offline cost to reach any point x ∈ BB′ is the norm ∥x∥,
whose value is symmetric around pt on BB′. So we can switch and pivot around B′ instead of
B, counter-clockwise. The semicircle curve from before has a corresponding semicircle curve
on the other side of pt. The fact the online algorithm went “below” the previous semicircle
curve means that it is now above/outside the new semicircle curve, and getting back onto the
curve only makes it more expensive for the online algorithm. Repeating this argument, the
algorithm should eventually reach a point pt at distance 1 from the origin after having moved
distance ≥ π/2.

The task for GPT-5. A subtle difficulty in the preceding argument lies in defining the
precise moment of pivot switching. If the switch occurs exactly when the algorithm crosses
below the semicircle, then at that moment the algorithm is still located on the semicircle, at
the location where it intersects the new semicircle. But then it might also try to go below the
new semicircle immediately, leading to a possibly infinite sequence of pivot switches at the
same instant, effectively freezing the process.

65

Kt

B′

pt

0

B

Figure IV.4: Illustration of π/2 lower bound idea. Figure made by GPT-5.

My initial plan for resolving this was to argue that the algorithm must move at a temporarily
faster speed in order to go below the semicircle, allowing the pivot switch to be delayed until its
speed decreases again; but formalizing this approach seemed technically cumbersome. GPT-5
suggested a much simpler and cleaner solution: trigger the switch once the algorithm is below
the semicircle at distance ≥ ϵ from pt. This avoids the freeze, and as ϵ → 0 we obtain the same
lower bound of π/2. This adjustment allows to make the argument rigorous and complete.
The idea seems obvious in hindsight, yet far more elegant than the overcomplicated solution
I was initially attempting. Since the interaction took place using Oxford’s ChatGPT Edu
license, due to sharing restrictions of this license a link to the full interaction is unfortunately
unavailable. An excerpt is shown in Figure IV.5.

Extension to higher dimension. Finally, I was hoping to extend the lower bound to
arbitrary dimension. My own approach (so far unsuccessful) was to first aim for an extension
of the above idea to three dimensions, but it does not seem obvious how to do this. I posed
the problem to GPT-5, with a prompt that included the lower bound idea above (without the
ϵ details) plus the following sentence:

[...] Try to generalize this idea for d = 2 in order to improve over
d lower bound for arbitrary dimension d.
the known

√

Within its first response, GPT-5 proposed an approach yielding a lower bound of π
2
√

⌊ d
2 ⌋ ≈
d. The idea is simple: Consider an orthogonal decomposition of Rd into ⌊d/2⌋ two-
1.11
dimensional components R2, and apply the π/2 lower bound one by one in the components.
Although there were errors in GPT-5’s proposed execution of this idea, it is not hard to
make rigorous. The full chat is available at [Coe25b] (containing also subsequent unsuccessful
attempts to push the ratio further). Notably, my initial focus on three dimensions might have
caused me to miss this simple decomposition idea.

q

IV.2.3 Reflection

GPT-5 assisted the research process on this problem at several stages. Perhaps the most
impressive part is its proof refuting the follow-the-leader algorithm, produced from a single

66

[Reasoned for 10m 38s]
[...]
A robust fix: ε-hysteresis switching
Pick any ε ∈ (0, 1). Run the adversary with the following rule.

• While pivoting around P (clockwise), keep pivoting as long as

uP (ϕ) = t(ϕ) − cos ϕ ≥ −ε.

When the first time τ with uP (τ ) = −ε occurs, switch pivot to

Q := the north-west intersection of Lτ with the unit circle,

and reverse the rotation direction.

• While pivoting around Q (counter-clockwise), keep pivoting as long as

uQ(ψ) := s(ψ) − cos ψ ≥ −ε

(here s(ψ) is the signed distance from Q along the current line). When the first
time with uQ = −ε occurs, switch to the new unit-circle intersection on that
line, and so on.

• Stop as soon as the current line becomes tangent at the current pivot (which

gives min w = 1).
Key geometry at a switch
At the switch time τ from P to Q, the midpoint M of [P, Q] is exactly p⋆(τ ) (the
work-function minimizer on Lτ ). Since uP (τ ) = −ε, the online point lies ε closer
to P than M along Lτ . Measured from the other endpoint Q, that means it lies ε

farther than M :

uQ(τ ) = +ε.

So after every switch we begin strictly outside the new Thales circle, with a
uniform gap ε. This removes the “immediately afterwards” ambiguity and prevents
instantaneous back-switching.
[...]

Figure IV.5: An excerpt from GPT-5 Pro’s response, in which it suggested the introduction of
a small parameter ε to avoid technicalities on when to switch in continuous time. (Note that
the notation used in this response differs from our explanations in this section.)

67

prompt without any guidance on how to approach the task. This result saved significant time
I might have otherwise spent unsuccessfully searching for an upper bound, and it is unclear if
I would have been able to come up with the lower bound, which contained several non-trivial
steps. The aid it provided for the lower bound on arbitrary algorithms was on the one hand
the inspiring appearance of the number π/2 in its reasoning trace, and on the other hand a
technical idea to resolve the “freezing” issue in the proof. While I would have presumably
found some solution to the freezing myself, it would have taken longer to formalize and likely
been less elegant. GPT-5’s extension to arbitrary dimension was simple, but beyond the scope
of my approach of focusing on three dimensions first.

Naturally, several limitations are also still apparent: it initially failed to understand how
viewing the problem in continuous time could yield stronger bounds, and upon later attempts
to push the lower bounds further, it presented arguments containing serious flaws. In the
proofs presented in this section, GPT-5’s responses also contained some errors, but these were
easy to fix for a human, overall accelerating the research process.

68

IV.3 Inequalities on subgraph counts in trees – Sébastien

Bubeck, Mark Sellke, and Steven Yin

We conduct here another experiment, this time in graph theory, on a problem formulated and
studied by the first author of this section, together with Nati Linial, in a 2013 preprint that
was later published in 2016 in the Journal of Graph Theory ([BL16]). The paper lists 7 open
problems, and the one we consider here is the first open problem, arguably the one we were
most interested in solving back in 2013:

• The open problem is about a conjectured infinite set of inequalities for subgraph counts

in trees, that if true would determine a certain convex set of interest.

• The follow-up paper [Bub+16] shows that, except possibly for the first two, none of
these inequalities are actually true. Moreover the main result of [Bub+16] is a proof
that the first inequality is indeed true. The second inequality is left as an open problem.

• Aided by somewhat sophisticated scaffolding (aimed at doing mathematics research),
GPT-5 was able to reprove the first inequality, and then build on this to also prove the
second (open) inequality. Both of GPT-5’s proofs are quite different from any of the
arguments in [BL16; Bub+16]. We present the re-proof of the first inequality in Section
IV.3.2 and the new proof of the second in Section IV.3.3.

• Both proofs below are entirely AI-generated, aside from minor editing for clarity. No
human input was needed beyond first asking to prove the first inequality, and then (with
the solution to the first inequality in context) asking to prove the second inequality. (A
few incorrect proofs were also generated and rejected by human checking.) See Figure
IV.7 and Figure IV.8 for the precise prompts.

• We moreover note that GPT-5 did not have access to web search in this experiment,
and did not seem to be aware of the existing proof in [Bub+16] (indeed the latter
paper has remained quite niche, with only 3 citations in 10 years). The pre-existing
(human-generated) proof rests on a somewhat cumbersome analysis, that analyzes several
different cases across 4 pages of calculation. In contrast GPT-5’s proof is short and
elegant, and based on a somewhat miraculous identity. The entire argument in Section
IV.3.2 fits in a page and a half.

We note that we first attempted to use this research scaffolding on top of GPT-5 to solve a
few Erdős problems, but with no success. The experiment described here was our first attempt
at a problem outside of Erdős’ problems.

IV.3.1 Paths, stars and wyes in trees

Let G be a finite tree. We consider the following subgraph counts (see also Figure IV.6):

• S is the number of induced star subgraphs on 5 vertices within G.

• P is the number of induced path subgraphs on 5 vertices within G.

• Y is the number of induced 5-vertex subtrees of the remaining isomorphism type within

G (i.e. 1 center vertex which is the endpoint of paths of lengths 1, 1, 2).

69

Figure IV.6: The three isomorphism types of trees with 5 vertices. TikZ figure produced by
GPT-5.

Let G be a finite tree graph. Let S be the number of induced
5-vertex star subtrees, P the number of induced 5-vertex path
subtrees, and Y the number of induced 5-vertex subtrees of the
remaining isomorphism type (one degree 3 central vertex, and 1 of
its 3 neighbors has a 2nd neighbor). Prove that Y ≤ 9 S + P + O(1).

Figure IV.7: Question posed to (a scaffolded version of) GPT-5 to reprove Theorem IV.3.1.
The answer it provided is written in Section IV.3.2, with minor edits for clarity.

Theorem IV.3.1 ([Bub+16]). For any finite tree G, one has

Y − 9S − P ≤ 6.

(IV.1)

Conjecture 1 ([BL16; Bub+16]). There exists some constant K > 0 such that for any finite
tree G, one has

29Y − 42P − 144S ≤ K.

(IV.2)

(cid:17)

(cid:16)

P
Y +S+P

S
Y +S+P ,

The motivation for the above problems is that the set of achievable asymptotic proportions
Y
in trees as Y + S + P → ∞ can be shown to be convex (in fact
Y +S+P ,
for general subtree counts); thus one would like to understand the linear constraints, and
Theorem IV.3.1 appeared to be a tight example. By contrast in general (dense) graphs, it is
known that the infinite-dimensional flag algebra semidefinite program of [Raz07] generates all
such inequalities, but that linear inequalities of the form above can still be computationally
undecidable [HN11].

In some sense Theorem IV.3.1 gives the “first face" of this convex set, while Conjecture 1
gives the “second face". We note that Conjecture 1 was also made in [BL16], together with an
infinite set of conjectured inequalities, one for each face, and those conjectured inequalities
were all disproven in [Bub+16] except for Conjecture 1 which we will show here to be actually
true.

Aside from being checked and rewritten for clarity, the new proofs below are entirely

AI-generated.

70

IV.3.2 New proof of Theorem IV.3.1

For v ∈ G let dv be the degree of v (number of neighbors) and:

xv = dv − 1,

Tv = X

xu,

u∼v

Av = Adv =

!
.

dv − 1
2

It is not hard to see that:

S = X
v∈G

!
,

dv
4

P =

1
2

X

h

v − X
T 2

i

,

x2
u

v∈G

u∼v

Y = X
v∈G

AvTv.

u∼v x2

Indeed the value of S is clear. For P , each summand counts the number of paths centered at v;
note that T 2
v counts the number of pairs of possibly identical length 2 paths with endpoint at
v, and P
u counts the number of such pairs which overlap (at u ∼ v). Thus T 2
u∼v x2
u
counts the number of ordered pairs of such paths which do not overlap, so dividing by 2
gives the number of paths centered at P . For Y , each summand again counts the number of
subgraphs centered at v. Again Tv counts the number of length 2 paths with endpoint at v;
given any such path, to make a Y it suffices to choose 2 of the remaining dv − 1 neighbors of
v, for a total of Adv options.

v − P

The first key claim is the identity

Y − 9S − P = X
v∈G

h

F (dv) −

(Tv − Av)2
2

i

(IV.3)

where F is the quartic polynomial:

F (k) =

(cid:1)2

(cid:0)k−1
2
2

+

k(k − 1)2
2

− 9

!
.

k
4

To prove (IV.3), firstly note that 9(cid:0)dv
4
up. The remaining identity to prove is:

(cid:1) = 9Sv and the terms T 2

v /2 and AvTv directly match

1
2

X

X

v

u∼v

x2
u

?=

(cid:1)2

(cid:0)xv
2
2

+

dvx2
v
2

−

A2
v
2

.

In fact, by definition (cid:0)xv
2

(cid:1)2 = A2

v, and by changing the order of summation one finds:
u = X
u = X
x2

u = X
x2

dvx2
v.

dux2

X

X

X

v

u∼v

u

v∼u

u

v

This establishes (IV.3).

Next, we have the miraculous identity for each v:

−

(Tv − Av)2
2

+F (dv) =

5
2

u∼v

X

(dv−du)−3(dv−2)−

dv(dv − 1)(dv − 3)(dv − 4)
4

−

Tv − Av − 2
2

!
.

(IV.4)
Given (IV.4) the rest of the proof is immediate. Indeed the first term on the right-hand side
telescopes when summing over v ∈ G (each term (dv − du) is counted once with sign 1 and

71

once with sign −1), and the third and fourth terms are never positive.1 Thus (IV.4) implies
that

Y − 9S − P ≤ 6|V (G)| − 3 X
v∈G

dv = 6

where the latter equality holds for all trees.

To verify (IV.4), we first compute (see e.g. Wolfram Alpha) that

Meanwhile

Finally

F (k) +

k(k − 1)(k − 3)(k − 4)
4

=

5k2
4

−

7k
4

+

1
2

.

(Tv − Av)2
2

−

Tv − Av − 2
2

!

=

5
2

(Tv − Av) − 3.

X

u∼v

(dv − du) = d2

v − X

(xu + 1) = d2

v − Tv − dv.

u∼v

Thus (IV.4) is equivalent to:

5d2
v
4

−

7dv
4

+

?=

1
2

5
2

(d2

v − Tv − dv) − 3dv + 6 +

5
2

(Tv − Av) − 3.

The Tv terms cancel, and grouping the remaining terms (without yet expanding Av) yields
the equivalent:

−

5d2
v
4

+

15dv
4

−

5
2

?= −

5Av
2

.

(IV.5)

Finally, expanding

Av =

!

dv − 1
2

=

d2
v − 3dv + 2
2

one finds that all terms in (IV.5) match. This completes the proof.

IV.3.3 Proof of Conjecture 1

As mentioned above, the work [BL16] actually made an infinite sequence of conjectures
parametrized by a positive integer C ≥ 2 (each corresponding to a face of the convex set of
achievable tree profiles), with the general statement that

C(2C2 + 7C + 7)Y − C(C + 1)(C2 + 3C + 4)P − 24(C + 1)(C + 2)S ≤ O(1)

for any fixed C. In addition to confirming the case C = 1, [Bub+16] actually disproved this for
C ≥ 3. The same proof-oriented scaffolding on top of GPT-5 was able to confirm the remaining
C = 2 case is true. The proof uses similar ideas but this time requires the non-positivity of a
quite complicated non-polynomial function on the integers.

Theorem IV.3.2. For any tree G, one has

29Y − 42P − 144S ≤ 504.

1Note that non-positivity of the third and fourth terms rely respectively on d and (Tv − Av) being integers.
The third in particular appears to be an inspired choice that would be difficult to find for a computer algebra
system, though it is perhaps natural given the equality cases (which contain vertices only of degree 1, 3, 4).

72

[Theorem IV.3.1 and Section IV.3.2 are put in context.]

More generally, we can state the inequality above like so

C(C+1)(C 2+3C+4) P +24(C+1)(C+2) S−C(2C 2+7C+7) Y ≥ cst

where the implied constant does not depend on G. The case C = 1
recovers the inequality above. Your task is to now prove or disprove

the case for C = 2.

Figure IV.8: Question posed to (a scaffolded version of) GPT-5 to prove Conjecture 1. The
answer it provided is written in Section IV.3.3, with minor edits for clarity.

Proof. We will double the inequality to be proved to conveniently reduce the number of
non-integer terms. Using the same formulas for S, P, Y as above, we see that

E := 58Y − 84P − 288S = X
v

(cid:0)58AvTv − 42T 2
v

(cid:1) + X
v

(cid:16)
42dv(dv − 1)2 − 288

!

(cid:17)

.

dv
4

We will rewrite this using the identities:

58AvTv − 42T 2

v = −29(Tv − Av)2 − 13T 2

v + 29A2
v,

−29(Tv − Av)2 = −58

−13T 2

v = −26

Tv − Av − 3
2
!

Tv − 3
2

− 91Tv + 156.

!

− 203(Tv − Av) + 348,

Substituting gives:

(cid:16)

E = X
v

− 58

Tv − Av − 3
2

!

− 26

!

Tv − 3
2

(cid:17)
− 294Tv + L(dv)

where

L(d) = 42d(d − 1)2 − 288

!

d
4

+ 29

!2

d − 1
2

+ 203

!

d − 1
2

+ 504.

Here we used 203 + 91 = 294 and 348 + 156 = 504. Similarly we above we have

(dv − du) = d2

v − Tv − dv

X

u∼v

and so

−294Tv = 294 X

(dv − du) − 294(d2

v − dv).

u∼v

73

The first term on the right-hand side telescopes to 0 from summing over v ∈ G. Thus we find
that

(cid:16)

E = X
v

− 58

Tv − Av − 3
2

where we have (again see Wolfram Alpha) that

!

− 26

Tv − 3
2

!

(cid:17)

+ J(dv)

J(d) = L(d) − 294(d2 − d) = −

19
4

d2 +

141
2

d3 −

1257
4

d2 +

33
2

d + 736.

The next step is to maximize each binomial coefficient binomial coefficient above, holding
Av fixed and letting Tv vary over the integers, so that each term in E becomes a univariate
function of d (since A = (cid:0)d−1
2

(cid:1)). Writing Yv = Tv − 3, we aim to minimize the quantity

58

Tv − Av − 3
2

!

+ 26

!

Tv − 3
2

= 42Y 2

v − (58Av + 42)Yv + 29Av(Av + 1).

(We are trying to upper bound E, but the above quantity is subtracted within E.) The
minimum is attained at Yv = 29Av+21

with value

42

29Av(Av + 1) −

(29Av + 212)
42

.

However since Tv and thus Yv must be integral, we actually find that

58

Tv − Av − 3
2

!

+ 26

!

Tv − 3
2

≥ 29Av(Av + 1) −

(29Av + 212)
42

+

s2
42

where s = min(r, 42 − r) and r = 29A + 21 (mod 42).
Yv = 29Av+21

to the nearest integer.) Note that A, hence r, hence s is a function of dv.

(I.e. s/42 is the distance from

42

Combining the above, we have bounded E from above by a sum of univariate functions of

dv:

E ≤ X
v∈G

J(dv) − m(dv)

where J was defined above and m(dv) = 29Av(Av +1)− (29Av+212)
in a tree, this means

42

+ s2

42 . Since P

v(dv −2) = −2

E − 1008 ≤ X
v∈G

(cid:16)

J(dv) − m(dv) + 504(dv − 2)

(cid:17)

.

It remains to show each term on the right-hand side is at most 0 for any value of dv. In fact
this does hold, with equality for all dv ∈ {1, 2, 4, 5}. This can be verified by an explicit exact
□
evaluation2, and completes the proof.

2If one drops the non-analytic s2/42 term, the resulting (larger) quartic polynomial alternates signs on the
sequence (0, 1, 3, 4, 6) and is negative at 6. Since it can have only 4 roots, this implies it is negative for all
dv ≥ 6. In particular, this means plugging in the values {0, 1, 2, 3, 4, 5, 6} suffices to confirm the numerical
claim. See also this Desmos plot.

74

IV.4 COLT problem on dynamic networks – Sébastien Bubeck,

Mark Sellke, and Steven Yin

We consider a random tree (Gt)t≥1 constructed iteratively as follows. At time t = 1 we begin
with a single vertex. At each subsequent time t + 1, a new vertex Yt+1 is added and attaches
to an existing parent vertex Xt+1 ∈ {Y1, . . . , Yt} chosen by a randomized preferential rule
described below.

Each vertex v independently receives a permanent “attractiveness” label a(v) ∈ {1, w}

with equal probability for the two options. The attractiveness-weighted degree of x ∈ Vt is
At(x) := X
y∼tx

a(y).

(Here ∼t denotes adjacency at time t.) The parent Xt+1 of Yt+1 is chosen according to

P(Xt+1 = x | Ft) =

At(x)
St

,

St := X
x∈Vt

At(x).

Here Ft = σ(G1, . . . , Gt) is the associated filtration.

1 + w

w

w

1

w

w

Figure IV.9: Illustration of the process at t = 3 with two vertices labelled w and one vertex
labelled 1. The attractiveness-weighted degree At of each of the existing vertices is displayed
next to it. Up to normalization, these are the attachment probabilities for the new fourth
vertex. TikZ figure produced by GPT-5.

Our goal will be to approximately recover w based on Gt for a single, large value of t
without observing the vertex labels. This was posed as an open problem3 at the 2012 Conference
on Learning Theory in [RC12], who observed that single-edge label statistics do not help
determine w (in fact their question asks for a bit more, see Remark IV.4.1). This type of
preferential attachment model was introduced in the seminal work [BA99] (in the case w = 1),
with the aim of better modeling the emergence in real networks and explaining observed
heavy-tailed behavior. Recovering w amounts, in this simplified toy model, to a qualitatively
interesting achievement: one has inferred the dynamical behavior of an evolving network while

3Importantly we note that this is the first COLT open problem that we tried with our scaffolded GPT-5.
We chose this paper somewhat randomly, with some preference for it due to the authors’ familiarity with
preferential attachment processes.

75

Consider a modified preferential attachment tree process, where
each vertex is also born with a label in {0, 1}, chosen uniformly
at random. The “attractiveness" of a vertex with label 1 is w > 0
and for a label 0 it is 1. The “attractive degree" of a vertex is
defined as the sum of attractiveness of its neighbors (so if w = 1
this is simply the usual degree). When a new vertex is added, it
connects to a random existing vertex with probability proportional
to their attractive degree. Based upon an observation of a large
unlabelled tree generated by this process, is it possible to identify w?
Either give an algorithm to do so, or prove an information theoretic
impossibility.

Figure IV.10: Question posed to (a scaffolded version of) GPT-5. It solved this problem by
coming up with and proving Theorem IV.4.1, and observing that this implies Corollary IV.4.1.
(In particular the decision to study the quantity L(t) required no human input!) The proofs
are provided below with minor edits.

only observing the network at just 1 time. Note that there is a redundancy in the model above:
reciprocating w does not change the law of (Gt)t≥1 because one can reverse all the labels.
Thus one should restrict to w ≥ 1, and we correspondingly posed the following question to (a
scaffolded version of) GPT-5 (see Figure IV.10).

GPT-5 was able to prove that w is indeed identifiable, stated in Corollary IV.4.1 below.
This is achieved by way of Theorem IV.4.1 below, which computes the limiting fraction f (w)
of vertices which are leaves at a given large time. To state the result, let L1(t) (resp. Lw(t))
be the number of leaves at time t whose parent (unique neighbor) has attractiveness 1 (resp.
w). Then the total number L(t) = L1(t) + Lw(t) of leaves at time t is observable without
observing the labels in Gt (while L1(t) and Lw(t) are not).

Theorem IV.4.1. For any w > 0, one has almost surely:

lim
t→∞

L(t)
t

= f (w) ≡

3
4

−

1
2(w + 2)

+

1
4(2w + 1)

=

3(w + 1)2
2(w + 2)(2w + 1)

.

(IV.6)

In fact, one has

lim
t→∞

L(1)(t)
t

=

w + 1
2(w + 2)

,

lim
t→∞

L(w)(t)
t

=

w + 1
2(2w + 1)

.

The function f defined in (IV.6) is easily seen to be strictly increasing, with

f ′(w) =

(cid:18)

1
2

1
(w + 2)2 −

1
(2w + 1)2

(cid:19)

> 0,

∀w > 1.

Identifiability of w is then immediate, as in the following Corollary.

Corollary IV.4.1. For any w ≥ 1 the point estimate ˆw(t) = f −1(L(t)/t) converges to w
in probability as t → ∞. Here f −1 : [2/3, 3/4] → [1, ∞] is the inverse of f |[1,∞], and we set
f −1(x) = 1 for x ≤ 2/3 and f −1(x) = ∞ for x ≥ 3/4.

76

Figure IV.11: A simulation of the asymptotic result (IV.6) in Theorem IV.4.1. The orange
plot shows the average (and empirical standard deviation) of the fraction of leaves in G105,
across 10 independent trials for each w. The function f (w) is graphed in blue on w ∈ [1, ∞),
where it strictly increases from f (1) = 2/3 to limw→∞ f (w) = 3/4. The code used to generate
this plot was written by GPT-5.

It is worth noting that the above type of algorithm provably fails in a related model of
evolving trees studied in [BV25] (in which attachment probabilities depend only on label
agreement and not on degree). In their model, the fraction of degree k vertices is asymptotically
2−k for all parameter values. This illustrates that GPT-5’s decision to focus on the quantity
L(t) is already non-obvious.

Remark IV.4.1. [RC12] in fact asks for a polynomial rate of convergence, i.e. an estimator
ˆw(t) satisfying P[| ˆw(t) − w| ≤ ε] ≥ 1 − δ for t polynomially large in (ε, δ). We believe a more
precise implementation of our approach suffices to show this stronger statement, but do not
pursue this here. [RC12] also considers a slightly more general setting in which incoming
vertices have probability p to be labeled w (which removes the convenient w ↔ 1/w symmetry).

Remark IV.4.2. While all major proof ideas below are due to GPT-5, a few details of proof
writing are human-supplied. Our scaffolded version of GPT-5 autonomously produced exactly
the high-level argument below and the same main references, but omitted the proof details for
certain intermediate assertions, stating (correctly) that they followed from routine stochastic
approximation arguments. When we asked (an unscaffolded) GPT-5 to provide more detail for
these latter arguments, it made several false starts, stating for example that [RS71, Theorem
1] directly implies Vt → 0 almost surely in the proof of Lemma IV.4.1 below, while it in fact
only gives convergence of Vt. After some human pushback, GPT-5 eventually came up with a
correct but unnecessarily complicated proof; we chose to present a more streamlined argument
(by observing that E[Vt] → 0).

77

IV.4.1 Proof Overview

We assume w ≥ 1 without loss of generality, and treat w as constant in all O(·) notation. Let

S(1)
t

:= X

At(x),

S(w)
t

:= X

At(x).

x:a(x)=1

x:a(x)=w

Then S(1)

t + S(w)

t = St, and

Rt ≡

S(w)
t
St

= P[a(Xt+1) = w]

is the chance for the next vertex to attach to a w-labelled vertex in Gt. Further, let

Dt := S(w)

t − 1

2 St =

t − S(1)
S(w)
2

t

= St · (Rt − 1

2 ),

µ = E[a(new vertex)] =

1 + w
2

(IV.7)

and set (with Et = E(Gt) the set of edges at time t)

Tt := X

a(u)a(v) =

{u,v}∈Et

t + wS(w)
S(1)
2

t

.

Then

E[a(Xt+1) | Ft] =

2Tt
St

.

(IV.8)

For notational convenience, given a discrete-time stochastic process (Zt)t≥1 we write ∆Zt =
Zt+1 − Zt.

The proof consists of two main steps. In the first we show Rt → 1/2 and St/t → w + 1
almost surely (see Lemmas IV.4.1 and IV.4.2). In the second we use this to analyze the
growth of L(1)(t) and L(w)(t). Both steps and especially the second employ classical stochastic
approximation results in the style of [RM51; RS71].

IV.4.2 One-Step Dynamics

When the (t + 1)-th vertex y = Yt+1 attaches to the random vertex x = Xt+1 ∈ {Y1, . . . , Yt},
we have

∆At(x) ≡ At+1(x) − At(x) = a(y),

∆At(y) = a(x),

and At(z) is unchanged for other vertices z ∈ {Y1, . . . , Yt}\{x}. Consequently

Using (IV.8) one obtains with some algebra that

∆St = a(x) + a(y).

Likewise,

E[∆St | Ft] = µ +

2Tt
St

=

Dt(w − 1)
St

+ (w + 1).

∆S(w)

t = 1{a(x) = w}a(y) + 1{a(y) = w}a(x),

and one can easily compute

We also trivially have

and |∆Rt| ≤ O(1/St).

E[∆Dt | Ft] = µ(cid:0)Rt − 1
2

(cid:1) =

µDt
St

.

2t ≤ St ≤ 2wt

78

(IV.9)

(IV.10)

(IV.11)

IV.4.3 Convergence of Rt and St
Lemma IV.4.1. limt→∞ Rt = 1
Proof. Define

2 almost surely.

We will show Vt → 0 holds almost surely. Letting Qt = Dt/St, one has

Vt := (Rt − 1

2 )2 = (Dt/St)2.

∆Vt = 2Qt∆Qt + (∆Qt)2.

Next it is easy to see by Taylor’s approximation theorem that the quotient rule holds approxi-
mately for discrete differences; since |∆S(w)

|, |∆St| ≤ O(1) we find:

t

St∆Dt − Dt∆St
S2
t
Combining the previous two displays, we find:

∆Qt =

+ O(S−2

t

).

∆Vt =

(cid:16)

2Dt
S2
t

∆Dt −

(cid:17)

Dt∆St
St

+ O(S−2

t

).

Using (IV.10)–(IV.9) yields

E[∆Vt | Ft] = −

4D2
t Tt
S4
t

+ O(S−2

t

) = −Vt

(cid:17)

(cid:16) 4Tt
S2
t

+ O(S−2

t

).

Since Tt ≍ St ≍ t, we get

By (IV.11), we have

E[∆Vt | Ft] ≤ −Ω(S−1

t

)Vt + O(S−2

t

).

X

t = ∞, X
S−1

t = ¯S(w) < ∞.
S−2

t≥1
The almost-supermartingale theorem of [RS71, Theorem 1] implies that limt→∞ Vt exists
almost surely. It remains to identify the limit. For this, letting Et = E[Vt], we obtain

t≥1

for a uniform constant c = c(w) > 0. Then

Et+1 ≤ (1 − cS−1

t

)Et + O(S−2

t

).

ET ≤ (E1 + ¯S(w)) · Y

(1 − cS−1

t

).

1≤t<T

This tends to 0 as T → ∞ because P S−1
for all t, its almost-sure t → ∞ limit must therefore equal 0. This completes the proof.

t = ∞. We conclude that Et → 0. Since Vt ∈ [0, 1]
□

Lemma IV.4.2. St/t → w + 1 almost surely.

Proof. Recalling (IV.9), we have

∆St =

Dt(w − 1)
St

+ (w + 1) + ξt+1

for a bounded martingale difference ξt+1. Since Dt/St → 0 from the previous lemma, Cesàro
averaging yields

The martingale SLLN gives 1
t

i=1 ξi → 0 almost surely. Thus St/t → w + 1.

□

(cid:16) Dt(w − 1)
St

1
t

t
X

i=1
Pt

+ (w + 1)

(cid:17)

→ w + 1.

79

IV.4.4 Stochastic Approximation for Leaf Fractions

We use the following stochastic approximation result, which gives an ODE limit for certain
stochastic processes. It follows by combining [Ben99, Propositions 4.1 and 4.2] (using q = 2
therein), and specializing to a 1-dimensional state space.

Theorem IV.4.2. Consider a scalar recursion

Zt+1 = Zt + γt+1(F (Zt) + εt+1 + δt+1)

driven by:

• Deterministic step sizes satisfying

γt → 0, X

γt = ∞, X

γ2
t < ∞.

t≥1

t≥1

• A martingale-difference sequence (εt)t≥1 adapted to a filtration Ft (i.e. E[εt+1|Ft] = 0)

and satisfying E[ε2

t+1|Ft] ≤ C for a deterministic constant C.

• Asymptotically vanishing adapted perturbations: δt → 0 almost surely.

• Locally Lipschitz F : R → R.

Suppose the ODE z′(T ) = F (z(T )) has a unique globally attracting equilibrium z⋆, i.e. for
all initial conditions z(0) ∈ R one has limT →∞ z(T ) = z⋆ for the solution to this ODE. Then
almost surely the analogous convergence holds for the discrete system:

lim
t→∞

Zt = z⋆.

Recall the definitions of L1(t), Lw(t) just above Theorem IV.4.1. It is easy to see that

E[∆L1 | Ft] =

S(1)
t − L1(t)
St

,

E[∆Lw | Ft] =

S(w)
t − wLw(t)
St

.

(IV.12)

Indeed L1(t)/St and Lw(t)/St are the conditional probabilities (given Ft) for Xt+1 to be a
leaf of label 1 and w. On these events Xt+1 is no longer a leaf in Gt+1.

The main quantities of interest will be the leaf fractions

xt = L1(t)/t,

yt = Lw(t)/t.

Since e.g. xt+1 = txt

t+1 when ∆L1(t) = 0, we have:

∆xt =

∆L1(t) − xt
t + 1

,

∆yt =

∆Lw(t) − yt
t + 1

.

We showed in Lemma IV.4.1 and IV.4.2 above that almost surely

(cid:16) S(1)
t
t

,

S(w)
t
t

,

St
t

(cid:17)

→

(cid:16) w + 1
2

,

w + 1
2

(cid:17)

, w + 1

.

(IV.13)

(IV.14)

80

In light of (IV.12) and (IV.13), we will apply Theorem IV.4.2 to xt and yt separately, with
γt+1 = 1/(t + 1) each in case. We use the vector fields

F1(x) =

1
2

− x −

x
1 + w

,

Fw(y) =

1
2

− y −

wy
1 + w

.

and the martingale noises

εX
t+1 := ∆L1(t) − E[∆L1(t) | Ft],

εY
t+1 := ∆Lw(t) − E[∆Lw(t) | Ft].

Focusing without loss of generality on the xt case, we obtain the recursion

xt+1 = xt + γt+1(F1(xt) + εX

t+1 + δX

t+1),

is defined to make equality hold, and one has δX

where δX
t → 0 almost surely as a consequence
t
of (IV.14). Since |∆L1| ≤ 1 almost surely, the required conditions on εX
t are clear. It is also
easy to see that the linear ODE x′(T ) = F1(x(T )) has a unique stable, globally attracting
fixed point, which is the zero of F1 given by

x⋆ =

w + 1
2(w + 2)

.

Thus we may apply Theorem IV.4.2 to conclude the almost sure limit:

xt/t →

w + 1
2(w + 2)

.

Identical reasoning shows that yt converges almost surely to the unique zero of Fw, i.e.

yt/t → y⋆ =

w + 1
2(2w + 1)

.

This completes the proof of Theorem IV.4.1.

81

Conclusion

AI models are well-known for assisting with routine tasks; what is less well-known is that
frontier models like GPT-5 are increasingly able to accelerate novel research. We highlighted
examples across a range of scientific disciplines showing GPT-5 assisting in ideation, powering
deep literature search, and even proposing and implementing proofs of (appropriately-sized)
open problems. None of this would have been possible just twelve months ago. The fact
that GPT-5 is already showing it can accelerate scientific research, combined with the rate of
improvement of frontier AI, suggests that the coming years will be an exhilarating time in
science and mathematics.

82

Acknowledgments

A. Lupsasca is supported in part by the National Science Foundation through the CAREER
award 2340457 and grant 2307888, and by the Simons Foundation through the grant SFI-
MPS-BH-00012593-09. This research was conducted during the period M. Sawhney served
as a Clay Fellow. C. Coester is funded by the European Union (ERC, CCOO, 101165139).
Views and opinions expressed are however those of the author(s) only and do not necessarily
reflect those of the European Union or the European Research Council. Neither the European
Union nor the granting authority can be held responsible for them. D. Unutmaz thanks Oral
Alpan, M.D. for critical reading of Section 3 of this report.

83

Bibliography

[Abu+24] H. Abu-Shawareb et al. “Achievement of Target Gain Larger than Unity in an
Inertial Fusion Experiment”. In: Phys. Rev. Lett. 132 (6 Feb. 2024), p. 065102.
doi: 10.1103/PhysRevLett.132.065102. url: https://link.aps.org/doi/10.
1103/PhysRevLett.132.065102.

[AM25]

Boris Alexeev and Dustin G Mixon. “Forbidden Sidon subsets of perfect difference
sets, featuring a human-assisted proof”. In: arXiv preprint arXiv:2510.19804
(2025).

[Alo24]

Noga Alon. “Graph-codes”. In: European Journal of Combinatorics 116 (2024),
p. 103880.

[Arg+19] C. J. Argue, Sébastien Bubeck, Michael B. Cohen, Anupam Gupta, and Yin Tat
Lee. “A Nearly-Linear Bound for Chasing Nested Convex Bodies”. In: Proceedings
of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA.
SIAM, 2019, pp. 117–122. doi: 10.1137/1.9781611975482.8.

[Arg+21] C. J. Argue, Anupam Gupta, Ziye Tang, and Guru Guruganesh. “Chasing Convex
Bodies with Linear Competitive Ratio”. In: J. ACM 68.5 (2021), 32:1–32:10. doi:
10.1145/3450349.

[AM04]

Stefano Atzeni and Jurgen Meyer-ter-Vehn. “The Physics of Inertial Fusion: Beam-
Plasma Interaction, Hydrodynamics, Hot Dense Matter”. In: Oxford University
Press (2004). Comprehensive reference for ICF plasma physics and alpha transport
models.

[Ban+20] Nikhil Bansal, Martin Böhm, Marek Eliás, Grigorios Koumoutsos, and Seeun
William Umboh. “Nested Convex Bodies are Chaseable”. In: Algorithmica 82.6
(2020), pp. 1640–1653. doi: 10.1007/S00453-019-00661-X.

[BC22]

[BA99]

[BSZ25]

[BV25]

Nikhil Bansal and Christian Coester. “Online Metric Allocation and Time-Varying
Regularization”. In: 30th Annual European Symposium on Algorithms, ESA.
Vol. 244. LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2022, 13:1–
13:13. doi: 10.4230/LIPICS.ESA.2022.13.

Albert-László Barabási and Réka Albert. “Emergence of scaling in random net-
works”. In: science 286.5439 (1999), pp. 509–512.

Guy Barzilai, Ohad Shamir, and Moslem Zamani. “Are Convex Optimization
Curves Convex?” In: arXiv preprint arXiv:2503.10138 (2025).

Anna Ben-Hamou and Vasiliki Velona. “Inference in balanced community modu-
lated recursive trees”. In: Bernoulli 31.1 (2025), pp. 457–483.

84

[Ben99]

Michel Benaïm. “Dynamics of Stochastic Approximation Algorithms”. In: Sémi-
naire de probabilités XXXIII. Vol. 1709. Lecture Notes in Mathematics. Springer,
1999, pp. 1–68.

[Bie+19] Marcin Bienkowski, Jaroslaw Byrka, Marek Chrobak, Christian Coester, Lukasz
Jez, and Elias Koutsoupias. “Better Bounds for Online Line Chasing”. In: 44th
International Symposium on Mathematical Foundations of Computer Science,
MFCS. Vol. 138. LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2019,
8:1–8:13. doi: 10.4230/LIPICS.MFCS.2019.8.
Thomas F. Bloom. https://www.erdosproblems.com/.

[Blo]

[BH92]

[Bra65]

[Bub+16]

[Bub+20]

[Bub+19]

[BL16]

[Coe25a]

[Coe25b]

H. S. Bosch and G. M. Hale. “Improved formulas for fusion cross-sections and
thermal reactivities”. In: Nuclear Fusion 32.4 (1992), pp. 611–631. doi: 10.1088/
0029-5515/32/4/I07.

S. I. Braginskii. “Transport Processes in a Plasma”. In: Reviews of Plasma Physics
1 (1965). Standard derivation of collisional transport coefficients for fully ionized
plasmas, pp. 205–311.

Sébastien Bubeck, Katherine Edwards, Horia Mania, and Cathryn Supko. “On
paths, stars and wyes in trees”. In: arXiv preprint arXiv:1601.01950 (2016).

Sébastien Bubeck, Bo’az Klartag, Yin Tat Lee, Yuanzhi Li, and Mark Sellke.
“Chasing Nested Convex Bodies Nearly Optimally”. In: Proceedings of the 2020
ACM-SIAM Symposium on Discrete Algorithms, SODA. SIAM, 2020, pp. 1496–
1508. doi: 10.1137/1.9781611975994.91.
Sébastien Bubeck, Yin Tat Lee, Yuanzhi Li, and Mark Sellke. “Competitively
Chasing Convex Bodies”. In: SIAM J. Comput. 52.on (2019), STOC19–339–
STOC19–353. doi: 10.1137/20M1312332.
Sébastien Bubeck and Nati Linial. “On the local profiles of trees”. In: Journal of
Graph Theory 81.2 (2016), pp. 109–119.

Christian Coester. Interaction with GPT-5 Pro. https://chatgpt.com/share/
690928df - de04 - 8009 - af7e - 84dd7a122ae4. [Online; accessed 20-November-
2025]. 2025.

Christian Coester. Interaction with GPT-5 Pro. https://chatgpt.com/share/
690928df - de04 - 8009 - af7e - 84dd7a122ae4. [Online; accessed 20-November-
2025]. 2025.

[Com+25] Samuel Compton, Gábor Lugosi, Jaouad Mourtada, Jiaqi Qian, and Nikita Zhivo-
tovskiy. “Geometry of Mixtures and KL-Optimal Density Estimation”. Manuscript,
version dated November 2025. Nov. 2025.

[DV01]

Thibault Damour and Alexander Vilenkin. “Gravitational wave bursts from
cusps and kinks on cosmic strings”. In: Phys. Rev. D 64 (2001), p. 064008. doi:
10.1103/PhysRevD.64.064008. arXiv: gr-qc/0104026.

[DMN25] Charles-Philippe Diez, Luis da Maia, and Ivan Nourdin. “Mathematical research
with GPT-5: a Malliavin-Stein experiment”. In: arXiv preprint arXiv:2509.03065
(2025).

[Erd69]

P Erdős. “Problems and results in chromatic graph theory”. In: Proof techniques
in graph theory (1969), pp. 27–35.

85

[EHP58]

[EG80]

[Erd61]

[Erd92]

[EHS82]

[FK25]

[FL93]

[GV87]

P Erdős, F Herzog, and G Piranian. “Metric properties of polynomials”. In: J.
Analyse Math 6.1 (1958), pp. 125–148.

P. Erdős and R. L. Graham. Old and new problems and results in combinatorial
number theory. Vol. 28. Monographies de L’Enseignement Mathématique [Mono-
graphs of L’Enseignement Mathématique]. Université de Genève, L’Enseignement
Mathématique, Geneva, 1980, p. 128.

Paul Erdős. “Some unsolved problems”. In: Magyar Tudományos Akadémia
Matematikai Kutató Intézet Közleményei (1961), pp. 221–254.

Paul Erdős. “Some of my favourite problems in various branches of combinatorics”.
In: Le Matematiche 47.2 (1992), pp. 231–240.

Paul Erdős, András Hajnal, and E Szemeredi. “On almost bipartite large chromatic
graphs.” In: Ann. Discrete Math 12 (1982), pp. 117–123.

Moran Feldman and Amin Karbasi. “Gödel Test: Can Large Language Models
Solve Easy Conjectures?” In: arXiv preprint arXiv:2509.18383 (2025).

Joel Friedman and Nathan Linial. “On Convex Body Chasing”. In: Discret.
Comput. Geom. 9 (1993), pp. 293–321. doi: 10.1007/BF02189324.

David Garfinkle and Tanmay Vachaspati. “Radiation From Kinky, Cuspless Cosmic
Loops”. In: Phys. Rev. D 36 (1987), p. 2229. doi: 10.1103/PhysRevD.36.2229.

[Geo+25] Bogdan Georgiev, Javier Gómez-Serrano, Terence Tao, and Adam Zsolt Wagner.

“Mathematical exploration and discovery at scale”. In: arXiv preprint arXiv:2511.02864
(2025).

[GY15]

[HL58]

[HN11]

[HL18]

[HK95]

[IX25]

[JR25]

[Ker63]

Parikshit Gopalan and Sergey Yekhanin. Personal communication. circa 2015.

R. Harm and Jr. L. Spitzer. “Transport phenomena in a completely ionized gas”.
In: Physics of Fluids 1.1 (1958), pp. 29–36. doi: 10.1063/1.1724337.

Hamed Hatami and Serguei Norine. “Undecidability of linear inequalities in graph
homomorphism densities”. In: Journal of the American Mathematical Society 24.2
(2011), pp. 547–565.

Walter K Hayman and Eleanor F Lingham. “Research Problems in Function
Theory (New Edition)”. In: arXiv preprint arXiv:1809.07200 (2018).

Mark B Hindmarsh and Thomas Walter Bannerman Kibble. “Cosmic strings”. In:
Reports on Progress in Physics 58.5 (1995), p. 477.

Paata Ivanisvili and Xinyuan Xie. “Counterexample to majority optimality in
NICD with erasures”. In: arXiv preprint arXiv:2510.20013 (2025).

Uijeong Jang and Ernest K Ryu. “Point Convergence of Nesterov’s Accelerated
Gradient Method: An AI-Assisted Proof”. In: arXiv preprint arXiv:2510.23513
(2025).

Roy P. Kerr. “Gravitational Field of a Spinning Mass as an Example of Alge-
braically Special Metrics”. In: Physical Review Letters 11 (1963), pp. 237–238.
doi: 10.1103/PhysRevLett.11.237.

86

[KLS02]

[LRW84]

[Lup25a]

[Lup25b]

[Lym62]

A. Khalfalah, S. Lodha, and E. Szemerédi. “Tight bound for the density of sequence
of integers the sum of no two of which is a perfect square”. In: Discrete Math.
256.1-2 (2002), pp. 243–255. issn: 0012-365X,1872-681X. doi: 10.1016/S0012-
365X(01)00435-6. url: https://doi.org/10.1016/S0012-365X(01)00435-6.

John Lewis, John Rossi, and Allen Weitsman. “On the growth of subharmonic
functions along paths”. In: Arkiv för Matematik 22.1 (1984), pp. 109–119.

Alex Lupsasca. Interaction with GPT-5 Pro. https://chatgpt.com/share/
68b006eb - ee0c - 8005 - 903f - bf92065d7e03. [Online; accessed 20-November-
2025]. 2025.

Alexandru Lupsasca. “Why there is no Love in black holes”. In: arXiv preprint
arXiv:2506.05298 (2025).

Jr. Lyman Spitzer. Physics of Fully Ionized Gases. 2nd. Classic reference for
Spitzer–Härm thermal conductivity and plasma transport theory. New York:
Interscience Publishers, 1962.

[Mar+96] M. M. Marinak, R. E. Tipton, O. L. Landen, T. J. Murphy, P. Amendt, S. W. Haan,
S. P. Hatchett, C. J. Keane, R. McEachern, and R. Wallace. “Three‚Äêdimensional
simulations of Nova high growth factor capsule implosion experiments”. In: Physics
of Plasmas 3.5 (May 1996), pp. 2070–2076. issn: 1070-664X. doi: 10.1063/1.
872004. eprint: https://pubs.aip.org/aip/pop/article- pdf/3/5/2070/
19192908/2070_1_online.pdf. url: https://doi.org/10.1063/1.872004.

[MW84]

[PY00]

[Pom59]

[Pom61]

[RC12]

[Raz07]

[RM51]

D. Mihalas and B. Weibel-Mihalas. Foundations of Radiation Hydrodynamics.
Source for gray radiation transport and escape probability formulations. New
York: Oxford University Press, 1984.

Christos H. Papadimitriou and Mihalis Yannakakis. “On the Approximability
of Trade-offs and Optimal Access of Web Sources”. In: Proceedings of the 41st
Annual Symposium on Foundations of Computer Science (FOCS 2000). IEEE,
2000, pp. 86–92.

Christian Pommerenke. “Über die Kapazität ebener Kontinuen”. In: Mathematis-
che Annalen 139.1 (1959), pp. 64–75.

Christian Pommerenke. “On metric properties of complex polynomials.” In: Michi-
gan Mathematical Journal 8.2 (1961), pp. 97–115.

Jan Ramon and Constantin Comendant. “Open Problem: Learning Dynamic
Network Models from a Static Snapshot”. In: Proceedings of the 25th Annual
Conference on Learning Theory. Ed. by Shie Mannor, Nathan Srebro, and Robert
C. Williamson. Vol. 23. Proceedings of Machine Learning Research. Edinburgh,
Scotland: PMLR, 25–27 Jun 2012, pp. 45.1–45.3. url: https://proceedings.
mlr.press/v23/ramon12.html.

Alexander A Razborov. “Flag algebras”. In: The Journal of Symbolic Logic 72.4
(2007), pp. 1239–1282.

Herbert Robbins and Sutton Monro. “A stochastic approximation method”. In:
Ann. Math. Stat. (1951), pp. 400–407.

87

[RS71]

[Sal25]

Herbert Robbins and David Siegmund. “A convergence theorem for non negative
almost supermartingales and some applications”. In: Optimizing methods in
statistics. Elsevier, 1971, pp. 233–257.

Adil Salim. “Accelerating mathematical research with language models: A case
study of an interaction with GPT-5-Pro on a convex analysis problem”. In: arXiv
preprint arXiv:2510.26647 (2025).

[Saw25a] Mehtaab Sawhney. Interaction with GPT-5 Pro. https://chatgpt.com/share/
68f124d4 - 30b4 - 8005 - b2d9 - fcb64fc1e4ea. [Online; accessed 20-November-
2025]. 2025.

[Saw25b] Mehtaab Sawhney. Interaction with GPT-5 Pro. https://chatgpt.com/share/
68ec50da - cf00 - 8005 - b5f6 - b683506e5853. [Online; accessed 20-November-
2025]. 2025.

[Sch25]

[Sel23]

[Spe25]

[SS24]

[Sze76]

[Teu73]

Robert Scherrer. Interaction with GPT-5 Pro. https://chatgpt.com/share/
690cb199 - 9018 - 8001 - a73d - dfd44da5d64d. [Online; accessed 20-November-
2025]. 2025.

Mark Sellke. “Chasing convex bodies optimally”. In: Geometric Aspects of Func-
tional Analysis: Israel Seminar (GAFA) 2020-2022. Springer. 2023, pp. 313–335.

Brian Keith Spears. Interaction with GPT-5 Pro. https://cdn.openai.com/
pdf/1c108ff1- 863e- 4a42- b970- a8474dcb8dd1/brian- spears- burn- wave-
gpt-convo.pdf. [Online; accessed 20-November-2025]. 2025.

S. David Storm and Robert J. Scherrer. “Gravitational radiation power spectrum
of Garfinkle-Vachaspati cosmic string loops”. In: Phys. Rev. D 110.8 (2024),
p. 083531. doi: 10.1103/PhysRevD.110.083531. arXiv: 2406.06743 [gr-qc].

E. Szemerédi. “On a problem of P. Erdős”. In: J. Number Theory 8.3 (1976),
pp. 264–270. issn: 0022-314X,1096-1658. doi: 10.1016/0022-314X(76)90003-2.
url: https://doi.org/10.1016/0022-314X(76)90003-2.

Saul A. Teukolsky. “Perturbations of a Rotating Black Hole. I. Fundamental
Equations for Gravitational, Electromagnetic, and Neutrino-Field Perturbations”.
In: Astrophysical Journal 185 (1973), pp. 635–648. doi: 10.1086/152444.

[Wal84]

Robert M. Wald. General Relativity. University of Chicago Press, 1984.

[Zhi25]

Nikita Zhivotovskiy. Interaction with GPT-5 Pro. https://chatgpt.com/share/
690db28a - 1b00 - 800c - b9e2 - 42490f586955. [Online; accessed 20-November-
2025]. 2025.

88

